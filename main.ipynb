{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-bf007d59-829d-470b-97f9-9cf7555e0f34",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Network Analysis Project \n",
    "#### Building an Recommendation System from Bipartite Networks\n",
    "---\n",
    "\n",
    "*Ludek Cizinsk√Ω (luci@itu.dk)*, *Louis Brandt (locb@itu.dk)*, *Lukas Rasocha (lukr@itu.dk)*, *Mika Senghaas (jsen@itu.dk)*, *Jacob Victor Enggaard Haahr (javh@itu.dk)*\n",
    "\n",
    "Course Manager: *Michele Coscia*\n",
    "\n",
    "Deadline: *22nd Decemeber 2021*\n",
    "\n",
    "Last Modified: *28th September 2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-fdb6c19e-a11d-4679-944e-fd5ef55563fb",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Assignment Description\n",
    "---\n",
    "You will have to deliver your project at the end of the course (deadline to be determined). You have to hand in a presentation (in PDF or Power Point format). It is mandatory to include the following information:\n",
    "\n",
    "- Basic network description of your data (what type of network it is, what does it represent, is it real or synthetically generated, etc). In practice, the result of project phase #1 (finding data).\n",
    "- Basic network statistics of your data (number of nodes, edges, clustering, degree distribution, etc). In practice, the result of project phase #2 (exploratory data analysis).\n",
    "- A clear statement of your research question, the result of project phase #3.\n",
    "- The analysis, results, and interpretation that allow you to answer your research question, the result of project phase #4.\n",
    "\n",
    "You're free to include this in the order you prefer and to add any additional information you deem necessary, but these are the mandatory components.\n",
    "\n",
    "The format of the oral is as follows: the students make a joint presentation followed by group questions. Subsequently the students are having individual examination with additional questions while the rest of the group is outside the room. The length of the oral will be 15 minutes X number of group members plus one -- for instance, a group of 6 will have 105 minutes ((6+1)*15). Which means you have 15 minutes of group exam plus 15 minutes of individual exam each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-e216af63-2d3d-472e-b191-f7e7b641052e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Environment Setup\n",
    "---\n",
    "This project uses Python libraries that are essential for the performed analysis. Make sure to have the dependencies listed in `requirements.txt` installed locally using the *Python Package Manager* `pip`. If installed, running the next code cell should install all relevant dependencies. Check documentation via the provided links:\n",
    "\n",
    "- [*NetworkX* Documentation](https://networkx.org/documentation/stable/reference/index.html)\n",
    "\n",
    "- [*NumPy* Quickstart](https://numpy.org/doc/stable/user/quickstart.html)\n",
    "- [*Matplotlib* Documentation](https://matplotlib.org/stable/tutorials/introductory/usage.html)\n",
    "- [*Pandas* Documentation](https://pandas.pydata.org/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-b4c91587-23d2-472b-8832-f6b1f8959a69",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-14aee654-90af-4fe0-bac9-74243ece0430",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2822,
    "execution_start": 1633939925517,
    "source_hash": "6b6b1295",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-fee89a0e-b2a2-47b5-9f57-9e9b5bc39684",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1476,
    "execution_start": 1633939928340,
    "source_hash": "a4893cee"
   },
   "outputs": [],
   "source": [
    "# network representation and algorithms\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "from pyvis.network import Network\n",
    "from networkx import linalg as nxla\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt           # basic plotting\n",
    "import seaborn as sns                          # advanced plotting\n",
    "import numpy as np                             # for representing n-dimensional arrays\n",
    "import scipy as sp                             # numerical computation\n",
    "import pandas as pd                            # dataframes\n",
    "from sklearn.linear_model import LinearRegression # Power fit\n",
    "\n",
    "\n",
    "\n",
    "# python standard library\n",
    "import json                                    # read/ write json\n",
    "import re                                      # regex search \n",
    "import os                                      # os operations\n",
    "import random                                  # randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-91fbf149-54bb-4103-8781-05f116ed2e73",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Flags\n",
    "\n",
    "Flags are used to control the run flow of the notebook when executed at once. This is useful, to prevent operations that should only produce a result once, from running multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-53bd492d-9bcd-4b38-aa38-b0a5ded0f905",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1633939929824,
    "source_hash": "76037a73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# section flags\n",
    "LOAD_DATA = True\n",
    "TRANSFORM_DATA = False\n",
    "COMPUTE_PROJECTIONS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-38436e51-9336-4e56-98ee-f9499a210276",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-22b35ef9-6971-4ab9-a837-1ac0c1f82e62",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1633939929836,
    "source_hash": "7708a6"
   },
   "outputs": [],
   "source": [
    "PATH_TO = {}\n",
    "PATH_TO['data'] = {}\n",
    "PATH_TO['data']['raw'] = 'data/raw'\n",
    "PATH_TO['data']['transformed'] = 'data/transformed'\n",
    "PATH_TO['data']['projections'] = 'data/projections'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-3d5d8dac-f82c-4a86-ba1d-5b84a95d7552",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Folder Structure\n",
    "\n",
    "Create relevant folders to read from and write to, if not yet existent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-08e3a7d1-6b4d-4695-a69d-33f4b46e040e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 484498,
    "execution_start": 1633939929852,
    "source_hash": "84ce9148",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate over path_to dict\n",
    "DIRS = ['data/raw', 'data/transformed', 'visualisations', '']\n",
    "# what folders do we need (?)\n",
    "for dir in DIRS:\n",
    "    # create if not existent\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-ad4ff0e0-c3b4-4f28-9e82-37ff1be70da7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## #01 Loading and Inspecting Raw Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-b9704f80-1b37-4859-8f1b-fec63d62a33a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 236,
    "execution_start": 1633939929892,
    "source_hash": "ec6986c"
   },
   "outputs": [],
   "source": [
    "if LOAD_DATA:\n",
    "    data = pd.read_csv(f\"{PATH_TO['data']['raw']}/data.txt\", delimiter=\":\", names=[\"user_id\", \"repo_id\"])\n",
    "    repos = pd.read_csv(f\"{PATH_TO['data']['raw']}/repos.txt\", delimiter=\":\", names=[\"repo_id\", \"meta_info\"])\n",
    "    lang = pd.read_csv(f\"{PATH_TO['data']['raw']}/lang.txt\", delimiter=\":\", names=[\"repo_id\", \"meta_info\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-a492c094-5e5a-4281-83a9-756fda0ba4da",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### `data.txt`\n",
    "\n",
    "This is the main dataset.  Each line is of the format `<user_id>`:`<repo_id>`\n",
    "which represents a user watching a repository.  There are 440,237 records\n",
    "in this file, each a single `user_id` and a single `repository_id` seperated by a colon. This file, thus, represents the bipartite graph of users following repositories as an edge list. The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-4ab9f25a-43ff-4672-b4ad-c12ccdfd85a3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 109,
    "execution_start": 1633939930134,
    "source_hash": "5d012fde",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Size of data.txt: {data.shape}\")\n",
    "print(f\"Number of Unique Users: {len(np.unique(data['user_id']))}\")\n",
    "print(f\"Number of Unique Repos: {len(np.unique(data['repo_id']))}\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-9e47b717-6ef6-4113-bead-d2eaaf1e8558",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### `repos.txt`\n",
    "\n",
    "This file lists out the 120,867 unique repositories using their id (`repo_id`) that are used in the `data.txt`\n",
    "set, providing the repository name, date it was created and (if applicable)\n",
    "the repository id that it was forked off of.  The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00015-d3554273-2334-404d-bf64-ad204ac9d8b3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 410,
    "execution_start": 1633939930220,
    "source_hash": "5be93304",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Size of repo.txt: {repos.shape}\")\n",
    "print(f\"ID's of data.txt and repos.txt match: {sum(np.unique(data['repo_id']) == np.unique(repos['repo_id'])) == 120867}\")\n",
    "print(f\"No. of Repos with forking info: {sum(repos['meta_info'].apply(lambda x: len(x.split(','))) == 3)}\")\n",
    "\n",
    "repos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-791b6bc2-f446-4171-a9a7-c6bddca22a5a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### `lang.txt`\n",
    "\n",
    "The last dataset included is the language breakdown data.  This lists  only contains 73,496 repositories, for which the language data was available. Each line of this file lists the repository id, then a comma delimited list of \n",
    "`<lang>`;`<lines>` entries containing each major language found and the number\n",
    "of lines of code for that language in the project.  The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-7ce72ccc-d5b0-43b5-bf04-34e1cea31259",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 65,
    "execution_start": 1633939930612,
    "source_hash": "5868930",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Size of repo.txt: {lang.shape}\")\n",
    "\n",
    "lang.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-0eb44930-382c-46fa-ad1e-b79e26933056",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## #02 Transforming Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-43286825-c940-40cf-8f2d-4eb121e22cab",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "###  Adding Class to Nodes in Edge List \n",
    "\n",
    "In the edge list `data.txt` the edges connecting a node of type *User* (from now referred to as $U$) to a node of type *Repository* (from now referred to as $R$) cannot be differentiated on each row. To read in the graph object correctly at a later point, we therefore add correct labeling into that dataframef "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-da5fbbdc-c809-498b-93a8-0b4fdccd65a0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1633939930653,
    "source_hash": "664ecae9"
   },
   "outputs": [],
   "source": [
    "if TRANSFORM_DATA:\n",
    "    data[\"user_id\"] = data[\"user_id\"].apply(lambda x: \"u\" + str(x))\n",
    "    data[\"repo_id\"] = data[\"repo_id\"].apply(lambda x: \"r\" + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-237c3979-38e5-48ec-9e31-58d20b850517",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Create Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00009-385e0618-75a4-475e-a7b1-e6239c11a736",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 484481,
    "execution_start": 1633939930653,
    "source_hash": "7d10674f"
   },
   "outputs": [],
   "source": [
    "if TRANSFORM_DATA:\n",
    "    # save repo metadata in dictionary\n",
    "    metadata = {}\n",
    "\n",
    "    # save metadata from repos.txt\n",
    "    for i in range(len(repos)):\n",
    "        repo_id, meta_info = repos.iloc[i]\n",
    "\n",
    "        # parse the meta info\n",
    "        if len(meta_info.split(',')) == 2:\n",
    "            repo_name, creation_date, forked_from_id = meta_info.split(',') + [None]\n",
    "        else:\n",
    "            # you had None here at index 2 as well (not intended?)\n",
    "            repo_name, creation_date, forked_from_id = meta_info.split(',')\n",
    "\n",
    "        # save to dict\n",
    "        metadata[f\"r{repo_id}\"] = {\n",
    "            \"repo_name\": repo_name,\n",
    "            \"creation_date\": creation_date,\n",
    "            \"forked_from_id\": forked_from_id\n",
    "        }\n",
    "    \n",
    "    # sve metadata from lang.txt\n",
    "    for i in range(len(lang)):\n",
    "        repo_id, lang_info = lang.iloc[i]\n",
    "       \n",
    "        # save it\n",
    "        if f\"r{repo_id}\" in metadata.keys():\n",
    "            metadata[f\"r{repo_id}\"][\"languages\"] = [l.split(';') for l in lang_info.split(',')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-fd9ea609-3cd3-4660-b9fd-b48e79c84d3d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Got this error when not checking if key in metadata dict, `KeyError: 'r59337'`. Kinda throws me off, since the metadata keys are initialised by iterating over repos.txt, which should contain all unique repos. that must mean that only is the lang not complete, but also contains records about repos that we don't have in our network data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-10ebb5ab-5813-42c7-b666-8f2752632fb7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Save Transformed Files\n",
    "\n",
    "For faster access later, the following code cell saves the transformed network data and meta data into the directory `data/transformed`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-9e2349d8-fd1b-495b-bfdf-0d2b8455fca7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 484482,
    "execution_start": 1633939930655,
    "source_hash": "382b4fa2"
   },
   "outputs": [],
   "source": [
    "if TRANSFORM_DATA:\n",
    "    # save edge list\n",
    "    data.to_csv(f\"{PATH_TO['data']['transformed']}/data.txt\", header=False, index=False)\n",
    "\n",
    "    # save metadata as json\n",
    "    with open(f\"{PATH_TO['data']['transformed']}/metadata.json\", \"w\") as fp:\n",
    "        json.dump(metadata, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-035fa351-f075-4cb1-a9ae-c9a1387d46f0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## #03 Load Graph \n",
    "\n",
    "After having transformed the raw data in a format that is easily usable to be loaded as a graph object, we load our graph. We are using `networkX` - the standard library in Python for representation, visualisation and computation on graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00014-8dc92a4c-3417-49a8-92f6-cbbbf2c35588",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2153,
    "execution_start": 1633939930702,
    "source_hash": "810161b3"
   },
   "outputs": [],
   "source": [
    "# load graph\n",
    "G = nx.read_edgelist(f\"{PATH_TO['data']['transformed']}/data.txt\", delimiter=\",\", comments='#', create_using=nx.Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00015-eb621fa4-d684-4259-a78f-1208ab2ba26d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 997,
    "execution_start": 1633939932861,
    "source_hash": "5fc5b167"
   },
   "outputs": [],
   "source": [
    "# load metadata\n",
    "with open(f\"{PATH_TO['data']['transformed']}/metadata.json\", \"r\") as fp:\n",
    "    metadata = json.load(fp)\n",
    "\n",
    "# add metadata to graph nodes\n",
    "for repo_id, vals in metadata.items():\n",
    "    to_add = {repo_id: vals}\n",
    "    nx.set_node_attributes(G, to_add, \"metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00016-8356792f-9c49-4b17-9393-1f09256b5606",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 484353,
    "execution_start": 1633939933864,
    "source_hash": "30d7300e"
   },
   "outputs": [],
   "source": [
    "# Show a random node's attributes\n",
    "G.nodes[\"r2\"][\"metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00017-43c8c4ad-b9ea-4a49-8037-911b85980efd",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## #04 Basic Network Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00032-c0476895-cf1a-4b70-8514-1e5b8b38344e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Numerical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00018-4abcad05-2059-4c2d-8a51-da164aaa09e2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1633939933873,
    "source_hash": "719f03d8"
   },
   "outputs": [],
   "source": [
    "# Store fundamental properties within pandas DF\n",
    "overview = pd.DataFrame(columns=[\"name\", \"is_bipartite\", \"is_directed\", \"is_weighted\", \"has_negative_weights\", \"selfloops_#\", \"nodes_#\", \"edges_#\", \"density\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00019-68f32ce5-95f3-457e-b358-f64b0b6216c8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2270,
    "execution_start": 1633939933887,
    "source_hash": "db990c54"
   },
   "outputs": [],
   "source": [
    "# save important data to dict\n",
    "G_info = dict()\n",
    "\n",
    "# fundamental stats\n",
    "G_info[\"name\"] = \"Github contest 2009\"\n",
    "G_info[\"number_of_users\"] = len([node for node in G.nodes() if node[0] == 'u'])\n",
    "G_info[\"number_of_repos\"] = len([node for node in G.nodes() if node[0] == 'r'])\n",
    "G_info[\"repo_highest_deg\"] = max([len(list(nx.classes.function.neighbors(G, node))) for node in G.nodes() if node[0] == 'r'])\n",
    "G_info[\"repo_highest_deg_name\"] = G.nodes[[node for node in G.nodes() if node[0] == 'r' and len(list(nx.classes.function.neighbors(G, node))) == G_info[\"repo_highest_deg\"]][0]][\"metadata\"][\"repo_name\"]\n",
    "G_info[\"user_highest_deg\"] = max([len(list(nx.classes.function.neighbors(G, node))) for node in G.nodes() if node[0] == 'u'])\n",
    "G_info[\"is_directed\"] = nx.is_directed(G)\n",
    "G_info[\"is_weighted\"] = nx.is_weighted(G)\n",
    "G_info[\"has_negative_weights\"] = nx.is_negatively_weighted(G)\n",
    "G_info[\"selfloops_#\"] = nx.number_of_selfloops(G)\n",
    "G_info[\"nodes_#\"] = nx.number_of_nodes(G)\n",
    "G_info[\"edges_#\"] = nx.number_of_edges(G)\n",
    "G_info[\"density\"] = nx.density(G)\n",
    "G_info[\"is_bipartite\"] = bool(nx.is_bipartite(G))\n",
    "\n",
    "# add it do the overview\n",
    "overview = overview.append(G_info, ignore_index=True)\n",
    "overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00036-39b21e1a-ea2b-4ef4-b205-572b9714d2c8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Top 10 repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-a4f41736-2948-48ac-874c-cd388e1ab01b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 732,
    "execution_start": 1633939936204,
    "source_hash": "72461587",
    "tags": []
   },
   "outputs": [],
   "source": [
    "repos_degree = sorted([(repo, G.degree(repo)) for repo in G.nodes() if repo[0] == 'r'], key=lambda x: x[1], reverse=True)[:10]\n",
    "repo_info = {\n",
    "    \"name\": [G.nodes[repo_tuple[0]][\"metadata\"][\"repo_name\"] for repo_tuple in repos_degree],\n",
    "    \"degree\": [repo_tuple[1] for repo_tuple in repos_degree],\n",
    "    \"technology_stack\": [G.nodes[repo_tuple[0]][\"metadata\"][\"languages\"] for repo_tuple in repos_degree]\n",
    "}\n",
    "top_10_repos = pd.DataFrame(repo_info)\n",
    "top_10_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00038-501f3617-e4bb-462c-9eb9-8f7554cad72a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 18,
    "execution_start": 1633939936922,
    "source_hash": "b9f97b35",
    "tags": []
   },
   "outputs": [],
   "source": [
    "set(G[\"u54540\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00036-3d1e473c-b254-45c0-bf0c-3f7a83fbf19f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-6fb934ad-b750-4029-8b9c-b229525184ca",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2551,
    "execution_start": 1633939936951,
    "source_hash": "d29b8593",
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_degree = [G.degree(n) for n in G.nodes() if n[0] == 'u']\n",
    "repos_degree = [G.degree(n) for n in G.nodes() if n[0] == 'r']\n",
    "\n",
    "user_degree, count_user_degree = np.unique(user_degree, return_counts=True)\n",
    "repos_degree, count_repos_degree = np.unique(repos_degree, return_counts=True)\n",
    "\n",
    "#Normalize\n",
    "count_user_degree = count_user_degree / sum(count_user_degree)\n",
    "count_repos_degree = count_repos_degree / sum(count_repos_degree)\n",
    "\n",
    "#CDF\n",
    "cdf_user_degree = user_degree.cumsum() / np.sum(user_degree)\n",
    "cdf_repos_degree = repos_degree.cumsum() / np.sum(repos_degree)\n",
    "\n",
    "#CCDF\n",
    "ccdf_user_degree = 1-cdf_user_degree\n",
    "ccdf_repos_degree = 1-cdf_repos_degree\n",
    "\n",
    "#Plotting on a log-log scale\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(13, 12))\n",
    "axes[0][0].plot(repos_degree, count_repos_degree,'.')\n",
    "axes[0][0].set_xlabel('Degree')\n",
    "axes[0][0].set_title('Repos degree')\n",
    "axes[0][0].set_ylabel('p(k)')\n",
    "axes[0][0].set_yscale('log')\n",
    "axes[0][0].set_xscale('log')\n",
    "\n",
    "\n",
    "axes[0][1].plot(user_degree, count_user_degree,'.')\n",
    "axes[0][1].set_title('User degree')\n",
    "axes[0][1].set_xlabel('Degree')\n",
    "axes[0][1].set_ylabel('p(k)')\n",
    "axes[0][1].set_yscale('log')\n",
    "axes[0][1].set_xscale('log')\n",
    "\n",
    "#CCDF\n",
    "#Repos ccdf\n",
    "axes[1][0].plot(repos_degree,ccdf_repos_degree)\n",
    "axes[1][0].set_title('Repos CCDF')\n",
    "axes[1][0].set_xlabel('x')\n",
    "axes[1][0].set_ylabel('p(k>=x)')\n",
    "axes[1][0].set_yscale('log')\n",
    "axes[1][0].set_xscale('log')\n",
    "\n",
    "#User ccdf\n",
    "axes[1][1].plot(user_degree,ccdf_user_degree)\n",
    "axes[1][1].set_title('USER CCDF')\n",
    "axes[1][1].set_xlabel('x')\n",
    "axes[1][1].set_ylabel('p(k>=x)')\n",
    "axes[1][1].set_yscale('log')\n",
    "axes[1][1].set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00038-1fd838d9-9510-4d37-ae0d-f79d6b1ab5ea",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Power law fit\n",
    "\n",
    "- needs some tuning, trying to visualize the shifted power law?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00039-9595dd92-59ef-4bd6-a029-f1835de01a8d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 475,
    "execution_start": 1633939939547,
    "source_hash": "3f80c4de",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Power law fit\n",
    "x_data = [repos_degree,user_degree,repos_degree,user_degree]\n",
    "y_data = [count_repos_degree,count_user_degree,ccdf_repos_degree,ccdf_user_degree]\n",
    "titles = ['Repos degree','User degree','Repos CCDF','User CCDF']\n",
    "\n",
    "#Transforming the scale and checking for 0 (where log is not defined)\n",
    "# 0 comes from the CCDF because there is a 0 probability that x will be larger than the absolutely largest \n",
    "# degree in the network\n",
    "\n",
    "x_trans_data = [np.where(i != 0, np.log10(i), 0) for i in x_data]\n",
    "y_trans_data = [np.where(i != 0, np.log10(i), 0) for i in y_data]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(10, 10), facecolor='w', edgecolor='k')\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(4):\n",
    "    x,y = x_trans_data[i].reshape((-1,1)), y_trans_data[i]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model = model.fit(x,y)\n",
    "\n",
    "    xs = np.linspace(np.min(x),np.max(x)+0.5,100)\n",
    "\n",
    "    ys = model.predict(xs.reshape((-1,1)))\n",
    "\n",
    "    axes[i].plot(x,y,'.')\n",
    "    axes[i].plot(xs,ys)\n",
    "    axes[i].set_title(titles[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00036-7142cda8-9b08-4aab-8b82-c2f624e4309f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## #05 Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-93bc004e-3614-4f7e-94cc-9eb25bb8fe2f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 30,
    "execution_start": 1633939940059,
    "source_hash": "165d96a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get V2, i.e., set of nodes representing repositories in acending sorted order by id\n",
    "repos = [node for node in G.nodes() if node[0] == 'r']\n",
    "users = [node for node in G.nodes() if node[0] == 'u']\n",
    "\n",
    "# Show the start\n",
    "print(repos[:5])\n",
    "print(users[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00044-cc714010-15ce-4497-89d1-02abaf85cf8d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8349,
    "execution_start": 1633939942256,
    "source_hash": "5ed5a4ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weight functions\n",
    "def simple_weight(G, u, v):\n",
    "    n_u, n_v = set(G[u]), set(G[v])\n",
    "    # len of set of intersection\n",
    "    return len(n_u & n_v) \n",
    "\n",
    "def jaccard(G, u, v):\n",
    "    n_u, n_v = set(G[u]), set(G[v])\n",
    "    # normalise len of set of intersectin by union\n",
    "    return len(n_u & n_v) / len(n_u | n_v)\n",
    "\n",
    "\n",
    "class VectorisedProjection:\n",
    "    \n",
    "    def __init__(self, G, repos, users):\n",
    "        self.metric = None\n",
    "        self.adj = nx.algorithms.bipartite.matrix.biadjacency_matrix(G, repos, users)\n",
    "        self.repo_map = {repos[i]: i for i in range(len(repos))}\n",
    "\n",
    "    def project(self, G, u, v):\n",
    "        \n",
    "        v_u = np.ravel(self.adj.getrow(0).todense().sum(axis=0))\n",
    "        v_v = np.ravel(self.adj.getrow(1).todense().sum(axis=0))\n",
    "        \n",
    "        if self.metric == 'simple_weight':\n",
    "            return np.sum((v_u + v_v) == 2)\n",
    "\n",
    "        elif self.metric == 'euclidean':\n",
    "            return sp.spatial.distance.euclidean(v_u, v_v)  \n",
    "\n",
    "        elif self.metric == 'normalised_euclidean':\n",
    "            return 1 / (np.sqrt(np.sum(np.power((v_u - v_v),2))) + 1)\n",
    "\n",
    "        elif self.metric == 'pearson':\n",
    "            return sp.stats.stats.pearsonr(v_u, v_v)[0] + 1\n",
    "\n",
    "        elif self.metric == 'cosine':\n",
    "            return 1 - sp.spatial.distance.cosine(v_u, v_v)\n",
    "\n",
    "        else:\n",
    "            print(\"Please specify one of the following metrics: ['simple_weight', 'euclidean', 'normalised_euclidean', 'pearson', 'cosine'].\")\n",
    "\n",
    "    \n",
    "def hyperbolic(G, u, v):\n",
    "    common = set(G[u]) & set(G[v])\n",
    "    return sum([1/(len(set(G[node])) - 1) for node in common])\n",
    "    \n",
    "def probs(G, u, v):  \n",
    "    common = set(G[u]) & set(G[v])\n",
    "    return sum([1/(len(set(G[node]))*len(set(G[u]))) for node in common])\n",
    "\n",
    "def heats(G, u, v):  \n",
    "    common = set(G[u]) & set(G[v])\n",
    "    return sum([1/(len(set(G[node]))*len(set(G[v]))) for node in common])\n",
    "\n",
    "def hybrid(G, u, v):  \n",
    "    common = set(G[u]) & set(G[v])\n",
    "    return sum([1/(len(set(G[node]))*len(set(G[u]))*len(set(G[v]))) for node in common])\n",
    "\n",
    "\n",
    "weight_functions = [simple_weight, jaccard, VectorisedProjection, hyperbolic, probs, heats]\n",
    "func_names = [\"Simple weight\", \"Jaccard\", \"Vectorised projection\", \"Hyperbolic\", \"Probs\", \"Heats\"]\n",
    "\n",
    "\n",
    "# test cases\n",
    "x = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
    "y = np.array([0, 0, 0, 0, 0, 1, 0, 0, 1, 1])\n",
    "\n",
    "\"\"\"\n",
    "print('simple_weight: ', np.sum((x + y) == 2))\n",
    "print('euclidean: ', np.sqrt(np.sum(np.power((x - y),2))))\n",
    "print('normalised euclidean: ', 1 / (np.sqrt(np.sum(np.power((x - y),2))) + 1))\n",
    "print('cosine: ', 1 - sp.spatial.distance.cosine(x, y))\n",
    "print('pearsn: ', sp.stats.stats.pearsonr(x, y)[0] + 1)\n",
    "\"\"\"\n",
    "\n",
    "u = 'r6'\n",
    "v = 'r3'\n",
    "\n",
    "for f, name in zip(weight_functions, func_names):\n",
    "    if f == VectorisedProjection:\n",
    "        f = VectorisedProjection(G, repos, users)\n",
    "        for metric in ['simple_weight', 'euclidean', 'normalised_euclidean', 'pearson', 'cosine']:\n",
    "            f.metric = metric\n",
    "            print(f\"{name} ({metric}): {f.project(G, u, v)}\")\n",
    "    else: \n",
    "        print(f\"{name}: {f(G, u, v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default benchmark, at this pace, it will compute within 10 min\n",
    "# Simple weight projection\n",
    "%timeit -n 2000 simple_weight(G, \"r1\", \"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate second method for projection, this time using pearson correlation of two vectors to measure edge weight\n",
    "vp = VectorisedProjection(G, repos, users)\n",
    "vp.metric = \"pearson\"\n",
    "\n",
    "# Run the test -  we see that the ratio with the previous method is: 690/2.93 ~ 235x, i.e., 235x more time (~40 h)\n",
    "%timeit -n 2000 vp.project(G, \"r1\", \"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00048-9c928f17-cf37-4b25-b790-03012da5a9b1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1633939950608,
    "output_cleared": true,
    "source_hash": "9d6d29e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if COMPUTE_PROJECTIONS:\n",
    "    weight_functions = [simple_weight, jaccard, VectorisedProjection, hyperbolic, probs, heats]\n",
    "    func_names = [\"simple_weight\", \"jaccard\", \"vectorised_projection\", \"hyperbolic\", \"probs\", \"heats\"]\n",
    "    for f, name in zip(weight_functions, func_names):\n",
    "        print(f\"Currently working on: {name}\")\n",
    "        \n",
    "        # Vectorised projections only\n",
    "        if f == VectorisedProjection:\n",
    "            \n",
    "            # Takes too long, therefore continue\n",
    "            continue\n",
    "            \n",
    "            # This will not be executed because of the continue\n",
    "            f = VectorisedProjection()\n",
    "            for metric in ['cosine', 'euclidean', 'normalised_euclidean', 'pearson']:\n",
    "                f.metric = metric\n",
    "                print(f\"\\n- {metric}\")\n",
    "                try:\n",
    "                    projected = bipartite.generic_weighted_projected_graph(G, nodes=repos, weight_function=f.project)\n",
    "                    nx.readwrite.gpickle.write_gpickle(projected, f\"{PATH_TO['data']['projections']}/{name}_{metric}.pickle\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Function: {name}_{metric} - failed because:\")\n",
    "                    print(e)\n",
    "                    print(\"-\"*10)\n",
    "        \n",
    "        # The rest of projections\n",
    "        else: \n",
    "            try:\n",
    "                projected = bipartite.generic_weighted_projected_graph(G, nodes=repos, weight_function=f)\n",
    "                nx.readwrite.gpickle.write_gpickle(projected, f\"{PATH_TO['data']['projections']}/{name}.pickle\")\n",
    "            except Exception as e:\n",
    "                print(f\"Function: {name} - failed because\")\n",
    "                print(e)\n",
    "                print(\"-\"*10)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load selected projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_simple = nx.readwrite.gpickle.read_gpickle(f\"{PATH_TO['data']['projections']}/simple_weight.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load testing graph to check simple computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_test = nx.path_graph(3)\n",
    "G_test.add_edge(2, 3, weight=5)\n",
    "G_test.add_edge(0, 1, weight=2)\n",
    "G_test.add_edge(1, 2, weight=4)\n",
    "G_test.edges.data(\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nx.linalg.graphmatrix.adjacency_matrix(G_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A*1/A.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A / A.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #05 Network backboning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple inspection of edge weight distribution. Below, you can see the first problem associated with using naive approach, i.e., to just get rid of nodes under certain nodes. The problem is broad weight distribution. (p. 333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = Counter([w for n1, n2, w in G_simple.edges.data(\"weight\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "plt.loglog(weights.keys(), weights.values(), 'o', alpha=0.5);\n",
    "plt.title(\"Edge weight distribution\")\n",
    "plt.xlabel(\"Edge weight\");\n",
    "plt.ylabel(\"# of edges\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets remove isolated nodes (as they will not be included in any recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes with no neighbours\n",
    "lonely_nodes = [node for (node,val) in G_simple.degree() if val == 0]\n",
    "G_simple.remove_nodes_from(lonely_nodes)\n",
    "#len(G_simple.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try the first backboning method: **Double stochastic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get adjacency matrix - rows: repos, columns: users\n",
    "G_simple_adj = nx.linalg.graphmatrix.adjacency_matrix(G_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn it into stochastic matrix\n",
    "G_simple_sto = G_simple_adj.multiply(1/G_simple_adj.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to get the doubly stochastic, we need to alternatively\n",
    "# normalize by row and column sum. We stop only when the deviation\n",
    "# from one is very little, thus the row/column sum is very close to\n",
    "# one. We also need to keep track of how many times we performed\n",
    "# the normalization. If we keep going back and forth between the\n",
    "# same two values, it means we're not converging.\n",
    "attempts = 0\n",
    "row_sums = G_simple_adj.sum(axis = 1)\n",
    "while np.std(G_simple_adj.sum(axis = 1)) > 1e-12:\n",
    "    print(np.std(G_simple_adj.sum(axis = 1)))\n",
    "    G_simple_adj = G_simple_adj.multiply(1/G_simple_adj.sum(axis = 1))\n",
    "    G_simple_adj = G_simple_adj.multiply(1/G_simple_adj.sum(axis = 0))\n",
    "    attempts += 1\n",
    "    if attempts > 1000:\n",
    "        print(\"Calculation didn't converge. The matrix cannot be made doubly stochastic. Aborting.\")\n",
    "        break\n",
    "\n",
    "print(\"Calculation converged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "5befc242-845e-4d7f-adb3-0e0bb747a3d1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
