{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7191bbd2",
   "metadata": {
    "cell_id": "00000-bf007d59-829d-470b-97f9-9cf7555e0f34",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Network Analysis Project \n",
    "#### Building an Recommendation System from Bipartite Networks\n",
    "---\n",
    "\n",
    "*Ludek Cizinsk√Ω (luci@itu.dk)*, *Louis Brandt (locb@itu.dk)*, *Lukas Rasocha (lukr@itu.dk)*, *Mika Senghaas (jsen@itu.dk)*, *Jacob Victor Enggaard Haahr (javh@itu.dk)*\n",
    "\n",
    "Course Manager: *Michele Coscia*\n",
    "\n",
    "Deadline: *22nd Decemeber 2021*\n",
    "\n",
    "Last Modified: *10th November 2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470c9b5",
   "metadata": {
    "cell_id": "00001-fdb6c19e-a11d-4679-944e-fd5ef55563fb",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Assignment Description\n",
    "---\n",
    "You will have to deliver your project at the end of the course (deadline to be determined). You have to hand in a presentation (in PDF or Power Point format). It is mandatory to include the following information:\n",
    "\n",
    "- Basic network description of your data (what type of network it is, what does it represent, is it real or synthetically generated, etc). In practice, the result of project phase #1 (finding data).\n",
    "- Basic network statistics of your data (number of nodes, edges, clustering, degree distribution, etc). In practice, the result of project phase #2 (exploratory data analysis).\n",
    "- A clear statement of your research question, the result of project phase #3.\n",
    "- The analysis, results, and interpretation that allow you to answer your research question, the result of project phase #4.\n",
    "\n",
    "You're free to include this in the order you prefer and to add any additional information you deem necessary, but these are the mandatory components.\n",
    "\n",
    "The format of the oral is as follows: the students make a joint presentation followed by group questions. Subsequently the students are having individual examination with additional questions while the rest of the group is outside the room. The length of the oral will be 15 minutes X number of group members plus one -- for instance, a group of 6 will have 105 minutes ((6+1)*15). Which means you have 15 minutes of group exam plus 15 minutes of individual exam each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef3ed2",
   "metadata": {
    "cell_id": "00000-e216af63-2d3d-472e-b191-f7e7b641052e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Environment Setup\n",
    "---\n",
    "This project uses Python libraries that are essential for the performed analysis. Make sure to have the dependencies listed in `requirements.txt` installed locally using the *Python Package Manager* `pip`. If installed, running the next code cell should install all relevant dependencies. Check documentation via the provided links:\n",
    "\n",
    "- [*NetworkX* Documentation](https://networkx.org/documentation/stable/reference/index.html)\n",
    "\n",
    "- [*NumPy* Quickstart](https://numpy.org/doc/stable/user/quickstart.html)\n",
    "- [*Matplotlib* Documentation](https://matplotlib.org/stable/tutorials/introductory/usage.html)\n",
    "- [*Pandas* Documentation](https://pandas.pydata.org/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6728b9",
   "metadata": {
    "cell_id": "00003-b4c91587-23d2-472b-8832-f6b1f8959a69",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fccc0de",
   "metadata": {
    "cell_id": "00003-14aee654-90af-4fe0-bac9-74243ece0430",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2822,
    "execution_start": 1633939925517,
    "source_hash": "6b6b1295",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9d800",
   "metadata": {
    "cell_id": "00001-fee89a0e-b2a2-47b5-9f57-9e9b5bc39684",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1476,
    "execution_start": 1633939928340,
    "source_hash": "a4893cee"
   },
   "outputs": [],
   "source": [
    "# network representation and algorithms\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "from pyvis.network import Network\n",
    "from networkx import linalg as nxla\n",
    "import powerlaw as pl                                            # powerlaw fits for degree distribution\n",
    "from IPython.display import display, Image, Markdown             # display images and markdown in jupyter\n",
    "\n",
    "# general data science libraries\n",
    "from matplotlib import pyplot as plt                            # basic plotting\n",
    "import seaborn as sns                                           # advanced plotting\n",
    "import numpy as np                                              # for representing n-dimensional arrays\n",
    "import scipy as sp                                              # numerical computation\n",
    "import pandas as pd                                             # dataframes\n",
    "\n",
    "# python standard library\n",
    "from time import time                                           # used for timing execution\n",
    "from datetime import date, datetime                             # get current data and time\n",
    "import json                                                     # read/ write json\n",
    "import re                                                       # regex search \n",
    "import os                                                       # os operations\n",
    "import random                                                   # randomness\n",
    "from collections import Counter                                 # efficient counting\n",
    "import contextlib\n",
    "\n",
    "# custom imports\n",
    "from cscripts import metrics\n",
    "from cscripts import plotting\n",
    "from cscripts import summarise\n",
    "from cscripts import backboning # michele\n",
    "from cscripts import github_api\n",
    "from cscripts import projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d108abd",
   "metadata": {},
   "source": [
    "### Set global style of plots\n",
    "\n",
    "Below you can specify global style for all plots or any other setups related to plots visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={\"xtick.bottom\" : True, \"ytick.left\" : True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030653f1",
   "metadata": {
    "cell_id": "00004-91fbf149-54bb-4103-8781-05f116ed2e73",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Flags\n",
    "\n",
    "Flags are used to control the run flow of the notebook when executed at once. This is useful, to prevent operations that should only produce a result once, from running multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b75eae0a",
   "metadata": {
    "cell_id": "00004-53bd492d-9bcd-4b38-aa38-b0a5ded0f905",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1633939929824,
    "source_hash": "76037a73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# section flags\n",
    "LOAD_DATA = True # Loads raw data for initial inspection\n",
    "TRANSFORM_DATA = False # Transforms raw data into more suitable format (Load data needs to be on as well)\n",
    "COMPUTE_PROJECTIONS = False\n",
    "GENERATE_SUMMARY_PROJ = False # Summary related to projections only\n",
    "DO_BACKBONING = False\n",
    "GENERATE_SUMMARY_PROJ_BACKB = False # Summary related to projected AND backboned graph\n",
    "SAVE_FIG = False # Do you want to save all generated figures?\n",
    "RANDOM_SAMPLE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312b0eb",
   "metadata": {
    "cell_id": "00008-38436e51-9336-4e56-98ee-f9499a210276",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c1c2f",
   "metadata": {
    "cell_id": "00002-22b35ef9-6971-4ab9-a837-1ac0c1f82e62",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1633939929836,
    "source_hash": "7708a6"
   },
   "outputs": [],
   "source": [
    "PATH_TO = {}\n",
    "PATH_TO['data'] = {}\n",
    "PATH_TO['data']['raw'] = 'data/raw'\n",
    "PATH_TO['data']['transformed'] = 'data/transformed'\n",
    "PATH_TO['data']['projections'] = 'data/projections'\n",
    "PATH_TO['data']['backboning'] = 'data/backboning'\n",
    "PATH_TO['data']['backboned_thresholded'] = 'data/backboned_thresholded'\n",
    "PATH_TO['data']['figures'] = 'data/figures'\n",
    "PATH_TO['data']['summaries'] = 'data/graph_summaries'\n",
    "PATH_TO['data']['metadata'] = 'data/metadata'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2493e5",
   "metadata": {
    "cell_id": "00010-3d5d8dac-f82c-4a86-ba1d-5b84a95d7552",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Folder Structure\n",
    "\n",
    "Create relevant folders to read from and write to, if not yet existent.\n",
    "An easy way to check if all the folders have been created is to use bash, go to the project folder and type \"tree\" it will give you an overview of the lists, check the readme for the folderstructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd957cca",
   "metadata": {
    "cell_id": "00011-08e3a7d1-6b4d-4695-a69d-33f4b46e040e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 484498,
    "execution_start": 1633939929852,
    "source_hash": "84ce9148",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate over path_to dict\n",
    "for path in PATH_TO['data'].values():\n",
    "    os.makedirs(path) if not os.path.exists(path) else print('Already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae44382",
   "metadata": {
    "cell_id": "00003-ad4ff0e0-c3b4-4f28-9e82-37ff1be70da7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## #01 Loading and Inspecting Raw Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236e3ee",
   "metadata": {
    "cell_id": "00005-b9704f80-1b37-4859-8f1b-fec63d62a33a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 236,
    "execution_start": 1633939929892,
    "source_hash": "ec6986c"
   },
   "outputs": [],
   "source": [
    "if LOAD_DATA:\n",
    "    data = pd.read_csv(f\"{PATH_TO['data']['raw']}/data.txt\", delimiter=\":\", names=[\"user_id\", \"repo_id\"])\n",
    "    repos = pd.read_csv(f\"{PATH_TO['data']['raw']}/repos.txt\", delimiter=\":\", names=[\"repo_id\", \"meta_info\"])\n",
    "    lang = pd.read_csv(f\"{PATH_TO['data']['raw']}/lang.txt\", delimiter=\":\", names=[\"repo_id\", \"meta_info\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0a00d",
   "metadata": {
    "cell_id": "00012-a492c094-5e5a-4281-83a9-756fda0ba4da",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### `data.txt`\n",
    "\n",
    "This is the main dataset.  Each line is of the format `<user_id>`:`<repo_id>`\n",
    "which represents a user watching a repository.  There are 440,237 records\n",
    "in this file, each a single `user_id` and a single `repository_id` seperated by a colon. This file, thus, represents the bipartite graph of users following repositories as an edge list. The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93533560",
   "metadata": {
    "cell_id": "00012-4ab9f25a-43ff-4672-b4ad-c12ccdfd85a3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 109,
    "execution_start": 1633939930134,
    "source_hash": "5d012fde",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Size of data.txt: {data.shape}\")\n",
    "print(f\"Number of Unique Users: {len(np.unique(data['user_id']))}\")\n",
    "print(f\"Number of Unique Repos: {len(np.unique(data['repo_id']))}\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f40505",
   "metadata": {
    "cell_id": "00014-9e47b717-6ef6-4113-bead-d2eaaf1e8558",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### `repos.txt`\n",
    "\n",
    "This file lists out the 120,867 unique repositories using their id (`repo_id`) that are used in the `data.txt`\n",
    "set, providing the repository name, date it was created and (if applicable)\n",
    "the repository id that it was forked off of.  The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7782813",
   "metadata": {
    "cell_id": "00015-d3554273-2334-404d-bf64-ad204ac9d8b3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 410,
    "execution_start": 1633939930220,
    "source_hash": "5be93304",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Size of repo.txt: {repos.shape}\")\n",
    "print(f\"ID's of data.txt and repos.txt match: {sum(np.unique(data['repo_id']) == np.unique(repos['repo_id'])) == 120867}\")\n",
    "print(f\"No. of Repos with forking info: {sum(repos['meta_info'].apply(lambda x: len(x.split(','))) == 3)}\")\n",
    "\n",
    "repos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b24930",
   "metadata": {
    "cell_id": "00016-791b6bc2-f446-4171-a9a7-c6bddca22a5a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### `lang.txt`\n",
    "\n",
    "The last dataset included is the language breakdown data.  This lists  only contains 73,496 repositories, for which the language data was available. Each line of this file lists the repository id, then a comma delimited list of \n",
    "`<lang>`;`<lines>` entries containing each major language found and the number\n",
    "of lines of code for that language in the project.  The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad05d4",
   "metadata": {
    "cell_id": "00017-7ce72ccc-d5b0-43b5-bf04-34e1cea31259",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 65,
    "execution_start": 1633939930612,
    "source_hash": "5868930",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Size of repo.txt: {lang.shape}\")\n",
    "\n",
    "lang.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a6076",
   "metadata": {
    "cell_id": "00018-0eb44930-382c-46fa-ad1e-b79e26933056",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## #02 Transforming Data\n",
    "---\n",
    "\n",
    "The goal of this section is to get the following two files:\n",
    "- data.txt is a textfile with the edge list containing our bipartite network\n",
    "- metadata.json is a json file containing metadata about each repository within our bipartite network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21676410",
   "metadata": {
    "cell_id": "00006-43286825-c940-40cf-8f2d-4eb121e22cab",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "###  Adding Class to Nodes in Edge List \n",
    "\n",
    "In the edge list `data.txt` the edges connecting a node of type *User* (from now referred to as $U$) to a node of type *Repository* (from now referred to as $R$) cannot be differentiated on each row. (e.g. 1:1 -> we need to denote that one on the left belongs to user and the other 1 to repository) To read in the graph object correctly at a later point, we therefore add correct labeling into that dataframe by adding `u` to user ids and `r` to repo ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83c514",
   "metadata": {
    "cell_id": "00007-da5fbbdc-c809-498b-93a8-0b4fdccd65a0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1633939930653,
    "source_hash": "664ecae9"
   },
   "outputs": [],
   "source": [
    "if TRANSFORM_DATA:\n",
    "    data[\"user_id\"] = data[\"user_id\"].apply(lambda x: \"u\" + str(x))\n",
    "    data[\"repo_id\"] = data[\"repo_id\"].apply(lambda x: \"r\" + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c14d5",
   "metadata": {
    "cell_id": "00008-237c3979-38e5-48ec-9e31-58d20b850517",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Create Metadata\n",
    "\n",
    "Here we combine `repos.txt` and `lang.txt` into one json file where keys are `repo ids` and values are corresponding `metadata`. Note that `repos.txt` should have info about all repos in our network, however `lang.txt` has only info about a subset of repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148d5a4",
   "metadata": {
    "cell_id": "00009-385e0618-75a4-475e-a7b1-e6239c11a736",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 484481,
    "execution_start": 1633939930653,
    "source_hash": "7d10674f"
   },
   "outputs": [],
   "source": [
    "if TRANSFORM_DATA:\n",
    "    # save repo metadata in dictionary\n",
    "    metadata = {}\n",
    "    repo_lang_mismatches = [] # Includes IDs of repos present in lang.txt but not in repos.txt\n",
    "\n",
    "    # save metadata from repos.txt\n",
    "    for i in range(len(repos)):\n",
    "        repo_id, meta_info = repos.iloc[i]\n",
    "\n",
    "        # parse the meta info\n",
    "        if len(meta_info.split(',')) == 2:\n",
    "            repo_name, creation_date, forked_from_id = meta_info.split(',') + [None]\n",
    "        else:\n",
    "            repo_name, creation_date, forked_from_id = meta_info.split(',')\n",
    "\n",
    "        # save to dict\n",
    "        metadata[f\"r{repo_id}\"] = {\n",
    "            \"repo_name\": repo_name,\n",
    "            \"creation_date\": creation_date,\n",
    "            \"forked_from_id\": forked_from_id\n",
    "        }\n",
    "    \n",
    "    # save metadata from lang.txt\n",
    "    for i in range(len(lang)):\n",
    "        repo_id, lang_info = lang.iloc[i]\n",
    "       \n",
    "        # save it\n",
    "        if f\"r{repo_id}\" in metadata.keys():\n",
    "            metadata[f\"r{repo_id}\"][\"languages\"] = [l.split(';') for l in lang_info.split(',')]\n",
    "        \n",
    "        # It means that there is a repository in lang.txt which is not present in repos.txt\n",
    "        else:\n",
    "            repo_lang_mismatches.append(repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773186e3",
   "metadata": {
    "cell_id": "00011-10ebb5ab-5813-42c7-b666-8f2752632fb7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Save Transformed Files\n",
    "\n",
    "For faster access later, the following code cell saves the transformed network data and meta data into the directory `data/transformed`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000df322",
   "metadata": {
    "cell_id": "00012-9e2349d8-fd1b-495b-bfdf-0d2b8455fca7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 484482,
    "execution_start": 1633939930655,
    "source_hash": "382b4fa2"
   },
   "outputs": [],
   "source": [
    "if TRANSFORM_DATA:\n",
    "    # save edge list\n",
    "    data.to_csv(f\"{PATH_TO['data']['transformed']}/data.txt\", header=False, index=False)\n",
    "    \n",
    "    # Save mismatches\n",
    "    with open(f\"{PATH_TO['data']['transformed']}/mismatches.csv\", \"w\") as fp:\n",
    "        fp.write(\",\".join([str(repoid) for repoid in repo_lang_mismatches]))\n",
    "\n",
    "    # save metadata as json\n",
    "    with open(f\"{PATH_TO['data']['transformed']}/metadata.json\", \"w\") as fp:\n",
    "        json.dump(metadata, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7991de",
   "metadata": {},
   "source": [
    "## #03 Network Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682eb59",
   "metadata": {},
   "source": [
    "### Solve problems with mismatches (repos.txt <> lang.txt)\n",
    "\n",
    "We start by double checking that indeed intersection of set of available repos and set of mismatch repos is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6575242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mismatches = repos present in lang.txt but not in repos.txt\n",
    "with open(f\"{PATH_TO['data']['transformed']}/mismatches.csv\", \"r\") as fp:\n",
    "    mismatches = fp.readlines()[0].split(',')\n",
    "\n",
    "# Load all repos present\n",
    "repos = pd.read_csv(f\"{PATH_TO['data']['raw']}/repos.txt\", delimiter=\":\", names=[\"repo_id\", \"meta_info\"])\n",
    "\n",
    "# Double check that indeed intersection of set of available repos and set of mismatch repos is empty\n",
    "len(set(repos[\"repo_id\"]) & set(mismatches)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d0b3a",
   "metadata": {},
   "source": [
    "According to the description repos in `data.txt` should be all present within `repos.txt`. In other words, difference between these two sets should be an empty set. Let's check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db568bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from data.txt\n",
    "datatxt = pd.read_csv(f\"{PATH_TO['data']['raw']}/data.txt\", delimiter=\":\", names=[\"user_id\", \"repo_id\"])\n",
    "\n",
    "# Check the above mentioned assumption\n",
    "len(set(datatxt[\"repo_id\"]) - set(repos[\"repo_id\"])) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07011dc3",
   "metadata": {},
   "source": [
    "Given the above two tests, we can safely conclude that mismatched **repos can be ignored.**\n",
    "\n",
    "### Solve problems nodes whose edges were randomly removed\n",
    "\n",
    "`Description of the problem:`\n",
    "\n",
    "I found out one important thing which we need to decide about. It is concerned with the stucture of our data. According to this website, the way the dataset was created was:\n",
    "- They retrieved all repo watches from their database (each edge then represents one watch in our network). Github was created back in 2008, therefore back then they ‚Äúonly‚Äù had around 0.5 million watches in db\n",
    "- From this set of watches, some of them were held back, i.e., removed from the dataset.\n",
    "- We were then provided a dataset of watches except from those that were withheld and from dataset called test.txt we know which users were impacted by this transformation. In other words, these users (roughly. 5000) have only a subset of their watches.\n",
    "- In total, there is around 56k users, and my question is to you what do we do with the 5k users about which we have incomplete information? (The reason why they withheld the watches is that the challenge of the contest was to predict these withheld watches‚Ä¶)\n",
    "\n",
    "`Proposed solution:`\n",
    "\n",
    "I think that it is reasonable to keep the affected users within the network and argue that we are working with a network which was adjusted by the approach described above, but most importantly, the above approach was random. In other words, the above approach for example did not focus on users which represent hubs. Another argument would be that this way, we do not lose information which we would have lost by dropping affected users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae87db0",
   "metadata": {
    "cell_id": "00013-035fa351-f075-4cb1-a9ae-c9a1387d46f0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## #04 Summarise Pre-Processed Bi-Partite Network\n",
    "---\n",
    "\n",
    "After having transformed the raw data in a format that is easily usable to be loaded as a graph object, we load our graph. We are using `networkX` - the standard library in Python for representation, visualisation and computation on graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5d28c",
   "metadata": {
    "cell_id": "00014-8dc92a4c-3417-49a8-92f6-cbbbf2c35588",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2153,
    "execution_start": 1633939930702,
    "source_hash": "810161b3"
   },
   "outputs": [],
   "source": [
    "# load graph\n",
    "G = nx.read_edgelist(f\"{PATH_TO['data']['transformed']}/data.txt\", delimiter=\",\", comments='#', create_using=nx.Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ec2d5",
   "metadata": {
    "cell_id": "00015-eb621fa4-d684-4259-a78f-1208ab2ba26d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 997,
    "execution_start": 1633939932861,
    "source_hash": "5fc5b167"
   },
   "outputs": [],
   "source": [
    "# load metadata\n",
    "with open(f\"{PATH_TO['data']['transformed']}/metadata.json\", \"r\") as fp:\n",
    "    metadata = json.load(fp)\n",
    "\n",
    "# add metadata to graph nodes\n",
    "for repo_id, vals in metadata.items():\n",
    "    to_add = {repo_id: vals}\n",
    "    nx.set_node_attributes(G, to_add, \"metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d1169",
   "metadata": {
    "cell_id": "00016-8356792f-9c49-4b17-9393-1f09256b5606",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 484353,
    "execution_start": 1633939933864,
    "source_hash": "30d7300e"
   },
   "outputs": [],
   "source": [
    "# Show a random node's attributes\n",
    "G.nodes[\"r2\"][\"metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbd3eab",
   "metadata": {
    "cell_id": "00032-c0476895-cf1a-4b70-8514-1e5b8b38344e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Fundamental information\n",
    "\n",
    "Below, you can find fundamental information about our bi-partite network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b955438",
   "metadata": {
    "cell_id": "00019-68f32ce5-95f3-457e-b358-f64b0b6216c8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2270,
    "execution_start": 1633939933887,
    "source_hash": "db990c54"
   },
   "outputs": [],
   "source": [
    "# Store fundamental properties within pandas DF\n",
    "overview = pd.DataFrame(columns=[\"name\", \"is_bipartite\", \"is_directed\", \"is_weighted\", \"selfloops_#\", \"nodes_#\", \"edges_#\", \"density\"])\n",
    "\n",
    "# save important data to dict\n",
    "G_info = dict()\n",
    "\n",
    "# fundamental stats\n",
    "G_info[\"name\"] = \"Github contest 2009\"\n",
    "G_info[\"number_of_users\"] = len([node for node in G.nodes() if node[0] == 'u'])\n",
    "G_info[\"number_of_repos\"] = len([node for node in G.nodes() if node[0] == 'r'])\n",
    "G_info[\"is_directed\"] = nx.is_directed(G)\n",
    "G_info[\"is_weighted\"] = nx.is_weighted(G)\n",
    "G_info[\"selfloops_#\"] = nx.number_of_selfloops(G)\n",
    "G_info[\"nodes_#\"] = nx.number_of_nodes(G)\n",
    "G_info[\"edges_#\"] = nx.number_of_edges(G)\n",
    "G_info[\"density\"] = nx.density(G)\n",
    "G_info[\"is_bipartite\"] = bool(nx.is_bipartite(G))\n",
    "\n",
    "# add it do the overview\n",
    "overview = overview.append(G_info, ignore_index=True)\n",
    "overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65733cc",
   "metadata": {
    "cell_id": "00036-39b21e1a-ea2b-4ef4-b205-572b9714d2c8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Top 10 repositories\n",
    "\n",
    "Below, we show 10 most popular repositories. Here are the interesting insights:\n",
    "- Each repository is written in Ruby, it is also the leading language in terms number of lines of code\n",
    "- The above perhaps corresponds to the fact that the top repository is rails which is a framework for web development for Ruby developers. Here is a relevant [article](https://syndicode.com/blog/why-is-ruby-still-our-choice-in-2020-2/) about Ruby popularity over time.\n",
    "- In addition, Github is developed using Ruby on Rails, perhaps that is where the popularity comes from. According to [this](https://gitstar-ranking.com/rails) site, Rails ranks 65th as of today.\n",
    "- Looking at other repositories, we can see that they correspond to web-development, e.g. authlogic or cucumber (platform for testing)\n",
    "- *(feel free to add more here)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b637f",
   "metadata": {
    "cell_id": "00037-a4f41736-2948-48ac-874c-cd388e1ab01b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 732,
    "execution_start": 1633939936204,
    "source_hash": "72461587",
    "tags": []
   },
   "outputs": [],
   "source": [
    "repos_degree = sorted([(repo, G.degree(repo)) for repo in G.nodes() if repo[0] == 'r'], key=lambda x: x[1], reverse=True)[:10]\n",
    "repo_info = {\n",
    "    \"id\": [repo_tuple[0] for repo_tuple in repos_degree],\n",
    "    \"name\": [G.nodes[repo_tuple[0]][\"metadata\"][\"repo_name\"] for repo_tuple in repos_degree],\n",
    "    \"degree\": [repo_tuple[1] for repo_tuple in repos_degree],\n",
    "    \"technology_stack\": [G.nodes[repo_tuple[0]][\"metadata\"][\"languages\"] for repo_tuple in repos_degree]\n",
    "}\n",
    "top_10_repos = pd.DataFrame(repo_info)\n",
    "top_10_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93170427",
   "metadata": {},
   "source": [
    "### Most used programming languages\n",
    "\n",
    "Below, we can see most used programming languages in terms of package development. Here are the notes/thoughts about the result below:\n",
    "- we should be aware that this metric can be easily skewed by outliers, i.e., you can have one large repository which will contribute a lot\n",
    "\n",
    "- C, Ruby, C++ as top 3 languages is not a surprise since nowadays C is still one of the most popular programming languages, with regards to Ruby, see the above section\n",
    "- *(feel free to add more)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bde20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the usage in dict such that: key is repo_name and value is number of lines of code\n",
    "lang_usage = dict()\n",
    "\n",
    "# Fill lang_usage\n",
    "for info in metadata.values():\n",
    "    if \"languages\" in info:\n",
    "        for lang_info in info[\"languages\"]:\n",
    "            lang_name, n_lines = lang_info\n",
    "            if lang_name in lang_usage:\n",
    "                lang_usage[lang_name] += int(n_lines)\n",
    "            else:\n",
    "                lang_usage[lang_name] = int(n_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for plotting\n",
    "# * First fit it to the pd dataframe\n",
    "langs = np.array(list(lang_usage.keys())).reshape(-1, 1)\n",
    "n_lines = np.array(list(lang_usage.values()), dtype=int).reshape(-1, 1)\n",
    "df_lang_usage = pd.DataFrame(np.concatenate((langs, n_lines), axis=1), columns = [\"Lang\", \"# of lines\"])\n",
    "df_lang_usage['# of lines'] = df_lang_usage['# of lines'].astype('int64')\n",
    "\n",
    "# * Second sorted by # of lines in descending order (largest first)\n",
    "sorteddf = df_lang_usage.sort_values([\"# of lines\"], axis = 0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup figure so you can plot inside it\n",
    "fig = plt.figure(figsize=(7, 6)) # create figure object with a (width,height)\n",
    "ax = fig.add_axes([0.1,0.2,0.8, 0.7]) # left, bottom, width, height (range 0 to 1)\n",
    "\n",
    "# Plot the data\n",
    "sns_plot = sns.barplot(x=\"Lang\", y=\"# of lines\", data=sorteddf[:10], ax=ax, palette=\"rocket\");\n",
    "ax.tick_params(axis='x', rotation=90);\n",
    "\n",
    "# Add axis labels\n",
    "ax.set_ylabel('Number of lines of code');\n",
    "ax.set_title(\"Most used programming languages\", weight=\"bold\");\n",
    "\n",
    "# Save plot\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(f\"{PATH_TO['data']['figures']}/most_used_lang.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621166f",
   "metadata": {
    "cell_id": "00036-3d1e473c-b254-45c0-bf0c-3f7a83fbf19f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Degree distribution\n",
    "\n",
    "*Disclaimer: The code for this section is adapted from [this source](https://www.networkatlas.eu/exercise.htm?c=6&e=4).*\n",
    "\n",
    "Start with creating a dataframe where one column represents possible degree of nodes, and the other count of nodes which have such degree. In addition, sort it. Do this for both type of nodes.\n",
    "\n",
    "#### Get df for repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3767fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_degree = [G.degree(n) for n in G.nodes() if n[0] == 'r']\n",
    "dd_repos = Counter(repos_degree) #getting the number of lists that have specific valuse\n",
    "dd_repos = pd.DataFrame(list(dd_repos.items()), columns = (\"k\", \"count\")).sort_values(by = \"k\")\n",
    "dd_repos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e255d",
   "metadata": {},
   "source": [
    "#### Get df for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_degree = [G.degree(n) for n in G.nodes() if n[0] == 'u']\n",
    "dd_user = Counter(user_degree)\n",
    "dd_user = pd.DataFrame(list(dd_user.items()), columns = (\"k\", \"count\")).sort_values(by = \"k\")\n",
    "dd_user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a8d15c",
   "metadata": {},
   "source": [
    "#### Visualize degree distribution (in terms of count) for both node types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7539d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define axs and figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.subplots_adjust(wspace=.3)\n",
    "\n",
    "# Plot the data\n",
    "for ax, data, which_category in zip(axs, [dd_repos, dd_user], [\"repositories\", \"users\"]):\n",
    "    sns.scatterplot(data=data, x=\"k\", y=\"count\", ax=ax, alpha=0.4)\n",
    "    ax.set_title(f\"Degree distribution for\\n{which_category} in terms of k\", weight = \"bold\")\n",
    "\n",
    "# Save the plot\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(f\"{PATH_TO['data']['figures']}/dd_bipartite_count.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068151b",
   "metadata": {},
   "source": [
    "#### Visualize degree distribution (in terms of count) on log-log scale for both node types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc03ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define axs and figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.subplots_adjust(wspace=.3)\n",
    "\n",
    "# Plot the data\n",
    "for ax, data, which_category in zip(axs, [dd_repos, dd_user], [\"repositories\", \"users\"]):\n",
    "    sns.scatterplot(data=data, x=\"k\", y=\"count\", ax=ax, alpha=0.4)\n",
    "    ax.set_title(f\"Degree distribution for\\n{which_category} in terms of k\", weight = \"bold\")\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "# Save the plot\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(f\"{PATH_TO['data']['figures']}/dd_bipartite_count_loglog.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a08edc",
   "metadata": {},
   "source": [
    "#### Get CCDF dataframe for repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad54fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdf_repos = dd_repos.sort_values(by = \"k\", ascending = False) #sort in descending order\n",
    "ccdf_repos[\"cumsum\"] = ccdf_repos[\"count\"].cumsum() \n",
    "ccdf_repos[\"ccdf\"] = ccdf_repos[\"cumsum\"] / ccdf_repos[\"count\"].sum()# normalize the data\n",
    "ccdf_repos = ccdf_repos[[\"k\", \"ccdf\"]].sort_values(by = \"k\") #sort by ascending degree\n",
    "ccdf_repos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a41f55",
   "metadata": {},
   "source": [
    "#### Get CCDF dataframe for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e52e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdf_users = dd_user.sort_values(by = \"k\", ascending = False)\n",
    "ccdf_users[\"cumsum\"] = ccdf_users[\"count\"].cumsum()\n",
    "ccdf_users[\"ccdf\"] = ccdf_users[\"cumsum\"] / ccdf_users[\"count\"].sum()\n",
    "ccdf_users = ccdf_users[[\"k\", \"ccdf\"]].sort_values(by = \"k\")\n",
    "ccdf_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5878388b",
   "metadata": {},
   "source": [
    "#### Visualize CCDF for both node types on a log-log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define axs and figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.subplots_adjust(wspace=.3)\n",
    "\n",
    "# Plot the data\n",
    "for ax, data, which_category in zip(axs, [ccdf_repos, ccdf_users], [\"repositories\", \"users\"]):\n",
    "    sns.lineplot(data=data, x=\"k\", y=\"ccdf\", ax=ax)\n",
    "    ax.set_title(f\"CCDF for {which_category}\", weight = \"bold\")\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "# Save the plot\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(f\"{PATH_TO['data']['figures']}/dd_bipartite_ccdf_loglog.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9bb91",
   "metadata": {
    "cell_id": "00038-1fd838d9-9510-4d37-ae0d-f79d6b1ab5ea",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Power law fit\n",
    "\n",
    "*Disclaimer: The code for this section is adapted from [this source](https://www.networkatlas.eu/exercise.htm?c=6&e=5).*\n",
    "\n",
    "Here we are checking if the network is a power law, this is checked because if the network is a power law as this would mean that there is afunctional relationship between the quantities.\n",
    "\n",
    "a ~ 2 is normal for a majority of networks which means that our network should only have one hub that is much larger than the rest\n",
    "\n",
    "k_min mentioned not sure if anything should be added abou it (page 107 in The Atlas for the Aspiring Network Scientist\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98105765",
   "metadata": {},
   "source": [
    "#### Compute power law fit for repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcda4a6",
   "metadata": {
    "cell_id": "00039-9595dd92-59ef-4bd6-a029-f1835de01a8d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 475,
    "execution_start": 1633939939547,
    "source_hash": "3f80c4de",
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pl.Fit(ccdf_repos[\"ccdf\"], discrete=True, fit_method=\"KS\")# fitting the data using the pl package\n",
    "k_min = ccdf_repos[ccdf_repos[\"ccdf\"] == results.power_law.xmin][\"k\"] #finding the degree value that corresponds to the probability in pl.xmin\n",
    "ccdf_repos[\"fit\"] = (10 ** results.power_law.Kappa) * (ccdf_repos[\"k\"] ** -results.power_law.alpha)\n",
    "print(\"Powerlaw CCDF Fit: %1.4f x ^ -%1.4f (k_min = %d)\" % (10 ** results.power_law.Kappa, results.power_law.alpha, k_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c14ae",
   "metadata": {},
   "source": [
    "#### Compute power law fit for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pl.Fit(ccdf_users[\"ccdf\"], discrete=True, fit_method=\"KS\")\n",
    "k_min = ccdf_users[ccdf_users[\"ccdf\"] == results.power_law.xmin][\"k\"]\n",
    "ccdf_users[\"fit\"] = (10 ** results.power_law.Kappa) * (ccdf_users[\"k\"] ** -results.power_law.alpha)\n",
    "print(\"Powerlaw CCDF Fit: %1.4f x ^ -%1.4f (k_min = %d)\" % (10 ** results.power_law.Kappa, results.power_law.alpha, k_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1fc241",
   "metadata": {},
   "source": [
    "#### Visualize the fit for both node types\n",
    "\n",
    "The conclusion from the both plots is that there is now way that either of them follows power law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define axs and figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.subplots_adjust(wspace=.3)\n",
    "\n",
    "# Plot the data\n",
    "for ax, data, which_category in zip(axs, [ccdf_repos, ccdf_users], [\"repositories\", \"users\"]):\n",
    "    sns.lineplot(data=data, x=\"k\", y=\"ccdf\", ax=ax)\n",
    "    sns.lineplot(data=data, x=\"k\", y=\"fit\", ax=ax)\n",
    "    ax.set_title(f\"Powerlaw fit for {which_category}\", weight = \"bold\")\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "# Save the plot\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(f\"{PATH_TO['data']['figures']}/power_law_fit_bipartite.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b12b8d",
   "metadata": {
    "cell_id": "00036-7142cda8-9b08-4aab-8b82-c2f624e4309f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## #05 Projections\n",
    "---\n",
    "\n",
    "In this section, we focus on computing projections of our bipartite network onto repository nodes. The reason why we decided to choose repository nodes is that it makes more sense in the context of our problem, i.e., predicting relevant repositories to follow for the given user. In other words, in the projected network, two nodes will be connected with an edge if they have common neighbors, in this case github users. We use several projections method where each method puts different weight $w$ to given edge.\n",
    "\n",
    "### Prepare projection methods\n",
    "\n",
    "In this section we implement selected projections from the book, you can see the implementation code in `cscripts/projections.py`. Below, we provide a description of implemented methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37010e3e",
   "metadata": {},
   "source": [
    "#### Summary of implemented projection methods\n",
    "\n",
    "##### Simple weights\n",
    "Simple weights works by identifying the number of common neighbors each pair of node has, which becomes the weight of the edge.\n",
    "\n",
    "##### Jaccard correction\n",
    "Takes a weighted link and applies the Jaccard correction by normalizing it with the union of the neighbor sets size.\n",
    "\n",
    "##### Hyperbolic weights\n",
    "In hyperbolic weights the hubs with a lot of connections contribute less to the weight than the ones outside of the hubs\n",
    "\n",
    "##### HeatS\n",
    "a type of resource allocation used to Generate the weights by doing random walks by initiating a endpoint for the random walk\n",
    "\n",
    "\n",
    "##### Hybrid\n",
    "A combination of the ProbS and HeatS methods\n",
    "\n",
    "*(Still missing some descriptions)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72018d8",
   "metadata": {},
   "source": [
    "### Test the implemented projection methods\n",
    "\n",
    "The purpose of the below test is to just show that the implemented methods run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4542a77",
   "metadata": {
    "cell_id": "00044-cc714010-15ce-4497-89d1-02abaf85cf8d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8349,
    "execution_start": 1633939942256,
    "source_hash": "5ed5a4ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define repos and users nodes\n",
    "repos = [node for node in G.nodes() if node[0] == 'r']\n",
    "users = [node for node in G.nodes() if node[0] == 'u']\n",
    "\n",
    "# Initiate projections object\n",
    "proj = projections.Projections(G, nodes={\"repos\": repos, \"users\": users})\n",
    "\n",
    "# Select random nodes from our bipartite network for which we want to do the test\n",
    "u = 'r6'\n",
    "v = 'r3'\n",
    "\n",
    "# Run the test\n",
    "results = {\"Method\": [], \"Score\": []}\n",
    "for name, info in proj.info.items():\n",
    "    \n",
    "    # Compute weight\n",
    "    w = proj.test_weight_func(name, info, u, v, \"repos\")\n",
    "    \n",
    "    # Show the result\n",
    "    results[\"Method\"].append(name)\n",
    "    results[\"Score\"].append(w)\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c875cd",
   "metadata": {},
   "source": [
    "### Test the performance of the implemented methods\n",
    "\n",
    "The goal of this section is to test performance of implemented methods. Or in other words, we want to figure out which methods are able to finish within a realistic timeframe.\n",
    "\n",
    "#### Default benchmark\n",
    "\n",
    "We tried to use `simple weight` projection method on our network, and it finished within `10 min`. Below, we look at the average time which it takes to compute weight for one edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default benchmark, at this pace, it will compute within 10 min\n",
    "# Simple weight projection\n",
    "%timeit -n 1000 proj.simple_weight(u, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6b281",
   "metadata": {},
   "source": [
    "#### Methods which use vectorization\n",
    "We found out that all methods which do not require vectorization are able to finish within reasonable timeframe. Below, we demonstrate perfomance of one of the selected methods which uses vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83062bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "adj = proj.get_sparse_adj_matrix(repos, users)\n",
    "v_u = proj.node_to_vector(repos, adj, u)\n",
    "v_v = proj.node_to_vector(repos, adj, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed7b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test\n",
    "%timeit -n 1000 proj.pearson(v_u, v_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7870d0b1",
   "metadata": {},
   "source": [
    "### Computation of projections\n",
    "\n",
    "Finally, after all testing and performance evaluation, we compute projection of our bi-partite network using following methods:\n",
    "- `Simple weight`\n",
    "- `Jaccard`\n",
    "- `Hyperbolic`\n",
    "- `Probs`\n",
    "- `Heats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define projections which you want to compute\n",
    "projection_functions_names = [\"simple_weight\", \"jaccard\", \"hyperbolic\", \"probs\", \"heats\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8a8d7",
   "metadata": {
    "cell_id": "00048-9c928f17-cf37-4b25-b790-03012da5a9b1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1633939950608,
    "output_cleared": true,
    "source_hash": "9d6d29e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if COMPUTE_PROJECTIONS:\n",
    "    metadata = {}\n",
    "    \n",
    "    for name in projection_functions_names:\n",
    "        metadata[name] = {}\n",
    "        metadata[name]['date'] = date.today().strftime('%d/%m/%y')\n",
    "        metadata[name]['time'] = datetime.now().strftime('%H:%M:%S')\n",
    "        metadata[name]['error'] = None\n",
    "        \n",
    "        print(f\"Currently working on: {name}\")\n",
    "        try:\n",
    "            start = time()\n",
    "            proj.project(which_n=\"repos\", which_p=name, savepath=f\"{PATH_TO['data']['projections']}/edge_list_format/{name}.edges\")\n",
    "            projected_time = round(time() - start, 2)\n",
    "            metadata[name]['projected_time'] = projected_time\n",
    "        except Exception as e:\n",
    "            print(f\"Function: {name} - failed because\")\n",
    "            metadata[name]['error'] = e\n",
    "            print(e)\n",
    "            print(\"-\"*10)\n",
    "        print()\n",
    "        metadata[name]['success'] = True\n",
    "     \n",
    "    summarise.write_metadata(metadata, filepath=PATH_TO['data']['metadata'], name='projections')\n",
    "    print('Saved Metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e7420",
   "metadata": {},
   "source": [
    "### Evaluating Projections\n",
    "\n",
    "In this section, we are trying to compute relevant statistics and visualise properties of the projections in a useful way in order to evaluate, which projection method might be the most reasonable to use for a recommendation system. We are using our custom function `generate_summary`, which generates a markdown file in the generated filepath for the desired unipartite graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7615fea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_SUMMARY_PROJ:\n",
    "    metadata = {}\n",
    "    \n",
    "    for name in projection_functions_names:\n",
    "        metadata[name] = {}\n",
    "        metadata[name]['date'] = date.today().strftime('%d/%m/%y')\n",
    "        metadata[name]['time'] = datetime.now().strftime('%H:%M:%S')\n",
    "        print('='*10)\n",
    "        print(f\"Projection Method: {name.replace('_',' ').title()}\\n\")\n",
    "        \n",
    "        # load edge list into graph\n",
    "        print(\"Loading Projection Graph\")\n",
    "        \n",
    "        start = time()\n",
    "        df = pd.read_csv(f\"{PATH_TO['data']['projections']}/edge_list_format/{name}.edges\",\n",
    "                        header=0,\n",
    "                        delimiter=\"\\t\")\n",
    "        G = nx.convert_matrix.from_pandas_edgelist(df, source=\"src\", target=\"trg\", edge_attr=\"weight\")\n",
    "        load_time = round(time() - start, 2)\n",
    "        metadata[name]['load_time'] = load_time\n",
    "\n",
    "        print(f\"Loaded in {load_time}s\\n\")\n",
    "        print(f\"Starting Markdown Generation\")\n",
    "        \n",
    "        start = time()\n",
    "        with contextlib.redirect_stdout(None): # surpress output\n",
    "            summarise.generate_markdown(G, filepath=f\"{PATH_TO['data']['summaries']}/projections\", name=name, plain=False)\n",
    "        execution_time = round(time() - start, 2)\n",
    "        metadata[name]['execution_time'] = execution_time\n",
    "            \n",
    "        print(f\"Finished in {execution_time}s. Summary saved to {PATH_TO['data']['summaries']}/projections/{name}\")\n",
    "        print('='*10 + '\\n')\n",
    "        metadata[name]['success'] = True\n",
    "    \n",
    "    summarise.write_metadata(metadata, filepath=PATH_TO['data']['metadata'], name='projections_summaries')\n",
    "    print('Saved Metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5254359c",
   "metadata": {},
   "source": [
    "## #06 Network backboning\n",
    "---\n",
    "\n",
    "Since our projected graphs are pretty dense, we need to use network backboning in order to filter out irrelevant edges.\n",
    "\n",
    "For backboning we have chosen the statistical approach as we use the Noise corrected and Disparity filter methods\n",
    "\n",
    "#### Disparity filter\n",
    "Disparity filter is a node centric approach, which means each node gets assigned their own threshold for which of its edges should be kept and which should be removed. Disparity filter take the average weight of the node as threshold\n",
    "\n",
    "#### Noise-corrected\n",
    "The noise corrected is very similar to the Disparity filter, Noise corrected is however an edge-centric approach where each edge has a different threshold for whether or not it should be kept, it does this by taking the average of the two connecting nodes weight and checks if it fulfills the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b34fe3",
   "metadata": {},
   "source": [
    "### Why can we not just filter out edges with weight less than certain threshold?\n",
    "\n",
    "Let's start with simple inspection of edge weight distribution.\n",
    "\n",
    "#### Get weights for graph obtained via simple weight projection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph obtained by simple weight projetion method\n",
    "G_simple = nx.readwrite.gpickle.read_gpickle(f\"{PATH_TO['data']['projections']}/pickle_format/simple_weight.pickle\")\n",
    "\n",
    "# Get weights within the graph\n",
    "weights = Counter([w for n1, n2, w in G_simple.edges.data(\"weight\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f9060",
   "metadata": {},
   "source": [
    "#### Plot the weights distribution\n",
    "\n",
    "Below, you can see the first problem associated with using naive approach, i.e., to just get rid of nodes under certain nodes. The problem is broad weight distribution. (p. 333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e847507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup figure so you can plot inside it\n",
    "fig = plt.figure(figsize=(6, 4)) # create figure object with a (width,height)\n",
    "ax = fig.add_axes([0.15,0.2,0.8, 0.7]) # left, bottom, width, height (range 0 to 1)\n",
    "\n",
    "# Plot the distribution\n",
    "weight_dist = sns.scatterplot(x=weights.keys(), y=weights.values(), alpha=0.5, ax=ax)\n",
    "weight_dist.set(xscale=\"log\", yscale=\"log\", xlabel=\"Edge weight\", ylabel=\"# of edges\");\n",
    "weight_dist.set_title(\"Edge weight distribution\", weight=\"bold\");\n",
    "\n",
    "# Save the figure\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(f\"{PATH_TO['data']['figures']}/weight_dist_simple_proj.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf0fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete G_simple from memory\n",
    "del G_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a866690f",
   "metadata": {},
   "source": [
    "### Setup for backboning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3761dd",
   "metadata": {},
   "source": [
    "In order to be able to use Michele's library, you need to provide in the specific format:\n",
    "- edge list\n",
    "- header must have columns src, trg, weight\n",
    "- should be tab separated\n",
    "- In case of undirected network, the edges have to be present in both directions with the same weights, or set triangular_input to True.\n",
    "\n",
    "With this in mind, I therefore saved our projections in this format. Note that by using edge list format, we lose nodes which are not connected to any other node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539bef7d",
   "metadata": {},
   "source": [
    "### Perform backboning for selected projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d930fc77",
   "metadata": {},
   "source": [
    "Now, that we have loaded the data in the proper format, we can do the network backboning. I chose the below two methods because they are computaionally viable. For instance Double stochastic method was not viable since we were not able to reach convergence. As Michele wrote in the book, this is likely because our network is sparse or because the amount of nodes in each type is not equal to each other. The other methods are very computationally intensive (we can try and see if it works, but I doubt it will run in reasonable time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "backboning_functions_names = ['noise_corrected', 'disparity_filter']\n",
    "backboning_functions = [backboning.noise_corrected, backboning.disparity_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_BACKBONING:\n",
    "    metadata = {}\n",
    "    for projection_name in projection_functions_names:\n",
    "        \n",
    "        # Header\n",
    "        print('='*10)\n",
    "        print(f\"Projection Method: {projection_name.replace('_',' ').title()}\\n\")\n",
    "        \n",
    "        # Load the data into a table --> needed format for the library. FYI: _, _ = nnnodes, nnedges\n",
    "        print('Loading Graph from edge list')\n",
    "        start = time()\n",
    "        table, _, _ = backboning.read(f\"{PATH_TO['data']['projections']}/edge_list_format/{projection_name}.edges\",\n",
    "                                         \"weight\",\n",
    "                                         triangular_input=False,\n",
    "                                         undirected=True,\n",
    "                                         consider_self_loops=False)\n",
    "        load_time = round(time() - start, 2)\n",
    "        print(f'Loaded in {load_time}s\\n')\n",
    "        \n",
    "        for name, backboning_function in zip(backboning_functions_names, backboning_functions):\n",
    "            \n",
    "            # Save fundamental metadata info\n",
    "            metadata[f'{projection_name} {name}'] = {}\n",
    "            metadata[f'{projection_name} {name}']['date'] = date.today().strftime('%d/%m/%y')\n",
    "            metadata[f'{projection_name} {name}']['time'] = datetime.now().strftime('%H:%M:%S')\n",
    "            metadata[f'{projection_name} {name}']['load_time'] = load_time\n",
    "            \n",
    "            \n",
    "            # Compute scores for backboning\n",
    "            print('Computing scores for backboning')\n",
    "            start = time()\n",
    "            backboning_function(table, undirected=True, return_self_loops=False).to_csv(f\"{PATH_TO['data']['backboning']}/{name}/{projection_name}.csv\", index=False)\n",
    "            execution_time = round(time() - start, 2)\n",
    "            metadata[f'{projection_name} {name}']['execution_time'] = execution_time\n",
    "            metadata[f'{projection_name} {name}']['success'] = True\n",
    "            print(f'Finished computing scores for backboning in {execution_time} s')\n",
    "    \n",
    "    # Save the metadata\n",
    "    summarise.write_metadata(metadata, PATH_TO['data']['metadata'], name='backboning')\n",
    "    print('Saved Metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5073b178",
   "metadata": {},
   "source": [
    "## #07 Analysis of projected and backboned graphs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f1e85",
   "metadata": {},
   "source": [
    "### Select a threshold to filter out edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b5b28",
   "metadata": {},
   "source": [
    "Below, select a signifficance score to filter out irrelevant edges from projected graph. The below `SIGNIF_THRS` as follows: `P-value` = 1 - `SIGNIF_THRS`. (Keep only edges whose `P-value` is higher than the `P-value` computed based on `SIGNIF_THRS`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef040a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNIF_THRS = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff0126",
   "metadata": {},
   "source": [
    "### Filter out edges and do analysis part\n",
    "\n",
    "Below, we first filter out irrelevant edges. Then compute summary of the given unipartite network and save it to markdown file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f765762",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_SUMMARY_PROJ_BACKB:\n",
    "    \n",
    "    metadata = {}\n",
    "\n",
    "    for projection_name in projection_functions_names:\n",
    "        \n",
    "        # Header\n",
    "        print('='*10)\n",
    "        print(f\"Projection Method: {projection_name.replace('_',' ').title()}\\n\")\n",
    "        \n",
    "        for name, backboning_function in zip(backboning_functions_names, backboning_functions):\n",
    "            \n",
    "            # Fundamental info\n",
    "            metadata[f'{projection_name} {name}'] = {}\n",
    "            metadata[f'{projection_name} {name}']['date'] = date.today().strftime('%d/%m/%y')\n",
    "            metadata[f'{projection_name} {name}']['time'] = datetime.now().strftime('%H:%M:%S')\n",
    "\n",
    "            # Filter\n",
    "            print(f'Backboning using {name}\\n')\n",
    "            start = time()\n",
    "            df = backboning.thresholding(\n",
    "                pd.read_csv(f\"{PATH_TO['data']['backboning']}/{name}/{projection_name}.csv\"),\n",
    "                SIGNIF_THRS)\n",
    "            backb_time = round(time() - start, 2)\n",
    "            metadata[f'{projection_name} {name}']['backboning_time'] = backb_time\n",
    "            print(f'Backboned in {backb_time} s\\n')\n",
    "            \n",
    "            # Load into a graph object\n",
    "            print(\"Loading the filtered edgelist into NX graph object\\n\")\n",
    "            start = time()\n",
    "            df[\"weight\"] = df[\"nij\"] # For naming consistency\n",
    "            G = nx.convert_matrix.from_pandas_edgelist(df, source=\"src\", target=\"trg\", edge_attr=[\"weight\", \"score\"])\n",
    "            load_time = round(time() - start, 2)\n",
    "            metadata[f'{projection_name} {name}']['load_time'] = load_time\n",
    "            print(f\"Loaded in {load_time} s\\n\")\n",
    "\n",
    "            # Analysis\n",
    "            print(f\"Starting Markdown Generation\\n\")\n",
    "            start = time()\n",
    "            with contextlib.redirect_stdout(None): # surpress output\n",
    "                \n",
    "                # * Define parameters\n",
    "                root_folder = f\"{PATH_TO['data']['summaries']}/backboned_projections\"\n",
    "                subfolder = f\"{projection_name}/{name}\"\n",
    "                filename = f\"{name}_{SIGNIF_THRS}\"\n",
    "                \n",
    "                # * Run the summary\n",
    "                summarise.generate_markdown(G, filepath=root_folder, name=subfolder, save_name=filename, plain=False)\n",
    "\n",
    "            execution_time = round(time() - start, 2)\n",
    "            metadata[f'{projection_name} {name}']['execution_time'] = execution_time\n",
    "            metadata[f'{projection_name} {name}']['success'] = True\n",
    "            print(f\"Finished in {execution_time} s. Summary saved to {root_folder}/{subfolder}\")\n",
    "            print('='*10 + '\\n')\n",
    "    \n",
    "    # Save metadata\n",
    "    summarise.write_metadata(metadata, filepath=PATH_TO['data']['metadata'], name='projections_backboning_summaries')\n",
    "    print('Saved Metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb276ed",
   "metadata": {},
   "source": [
    "## #08 Similarity algorithms\n",
    "\n",
    "In this section we focus on utilizing our gained knowledge from previous sections and we try to find optimal algorithms for finding similar repositories.\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20336e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f28c81",
   "metadata": {},
   "source": [
    "# #09 Visualization - Gephi\n",
    "\n",
    "## Edge induced random sampling\n",
    "\n",
    "- because selecting nodes randomly will disregard hubs since there is a way larger probability of choosing a non-hub, whereas choosing random edges does not have that issue since more edges are connected to hubs (read more in the book in chapter Network Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa81b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RANDOM_SAMPLE:\n",
    "    #SETUP\n",
    "    name = 'jaccard'\n",
    "    sampled_graph_size = 10000\n",
    "\n",
    "    #SAMPLING\n",
    "    edge_list = pd.read_csv(f'data/projections/edge_list_format/{name}.edges', sep='\\t')\n",
    "    edge_list = np.array(edge_list)\n",
    "    random_edges = np.random.choice(edge_list.shape[0], size=sampled_graph_size, replace=False)\n",
    "    sampled_edges= edge_list[random_edges,:]\n",
    "    pd.DataFrame(sampled_edges).to_csv(f\"data/sampled/{name}_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c9b3a",
   "metadata": {},
   "source": [
    "### Jaccard random sample\n",
    "- #Edges 10000\n",
    "- #Nodes 10986\n",
    "\n",
    "- #AVG degree 0.91\n",
    "\n",
    "- #Number of weakly connected components 1688\n",
    "\n",
    "#### Interpretation\n",
    "- Purple colour edge - large edge weight (gradient)\n",
    "- Blue colour edge - small edge weight (gradient)\n",
    "- Blue colour node - large degree (gradient)\n",
    "- Green colour node - small degree (gradient)\n",
    "- Degree distribution follows the theoretical degree distribution (can be seen in data/sampled/degree_distribution_sample.png)\n",
    "\n",
    "![alt text](data/sampled/screenshot_133054.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0316bee1",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "5befc242-845e-4d7f-adb3-0e0bb747a3d1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
