{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-bf007d59-829d-470b-97f9-9cf7555e0f34",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Network Analysis Project \n",
    "#### Building an Recommendation System from Bipartite Networks\n",
    "---\n",
    "\n",
    "*Ludek Cizinsk√Ω (luci@itu.dk)*, *Louis Brandt (locb@itu.dk)*, *Lukas Rasocha (lukr@itu.dk)*, *Mika Senghaas (jsen@itu.dk)*, *Jacob Victor Enggaard Haahr (javh@itu.dk)*\n",
    "\n",
    "Course Manager: *Michele Coscia*\n",
    "\n",
    "Deadline: *22nd Decemeber 2021*\n",
    "\n",
    "Last Modified: *10th November 2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-fdb6c19e-a11d-4679-944e-fd5ef55563fb",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Assignment Description\n",
    "---\n",
    "You will have to deliver your project at the end of the course (deadline to be determined). You have to hand in a presentation (in PDF or Power Point format). It is mandatory to include the following information:\n",
    "\n",
    "- Basic network description of your data (what type of network it is, what does it represent, is it real or synthetically generated, etc). In practice, the result of project phase #1 (finding data).\n",
    "- Basic network statistics of your data (number of nodes, edges, clustering, degree distribution, etc). In practice, the result of project phase #2 (exploratory data analysis).\n",
    "- A clear statement of your research question, the result of project phase #3.\n",
    "- The analysis, results, and interpretation that allow you to answer your research question, the result of project phase #4.\n",
    "\n",
    "You're free to include this in the order you prefer and to add any additional information you deem necessary, but these are the mandatory components.\n",
    "\n",
    "The format of the oral is as follows: the students make a joint presentation followed by group questions. Subsequently the students are having individual examination with additional questions while the rest of the group is outside the room. The length of the oral will be 15 minutes X number of group members plus one -- for instance, a group of 6 will have 105 minutes ((6+1)*15). Which means you have 15 minutes of group exam plus 15 minutes of individual exam each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-e216af63-2d3d-472e-b191-f7e7b641052e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Environment Setup\n",
    "---\n",
    "This project uses Python libraries that are essential for the performed analysis. Make sure to have the dependencies listed in `requirements.txt` installed locally using the *Python Package Manager* `pip`. If installed, running the next code cell should install all relevant dependencies. Check documentation via the provided links:\n",
    "\n",
    "- [*NetworkX* Documentation](https://networkx.org/documentation/stable/reference/index.html)\n",
    "\n",
    "- [*NumPy* Quickstart](https://numpy.org/doc/stable/user/quickstart.html)\n",
    "- [*Matplotlib* Documentation](https://matplotlib.org/stable/tutorials/introductory/usage.html)\n",
    "- [*Pandas* Documentation](https://pandas.pydata.org/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-b4c91587-23d2-472b-8832-f6b1f8959a69",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-14aee654-90af-4fe0-bac9-74243ece0430",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2822,
    "execution_start": 1633939925517,
    "source_hash": "6b6b1295",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-fee89a0e-b2a2-47b5-9f57-9e9b5bc39684",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1476,
    "execution_start": 1633939928340,
    "source_hash": "a4893cee"
   },
   "outputs": [],
   "source": [
    "# network representation and algorithms\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "from pyvis.network import Network\n",
    "from networkx import linalg as nxla\n",
    "import powerlaw as pl                                            # powerlaw fits for degree distribution\n",
    "from IPython.display import display, Image, Markdown             # display images and markdown in jupyter\n",
    "\n",
    "# general data science libraries\n",
    "from matplotlib import pyplot as plt                            # basic plotting\n",
    "import seaborn as sns                                           # advanced plotting\n",
    "import numpy as np                                              # for representing n-dimensional arrays\n",
    "import scipy as sp                                              # numerical computation\n",
    "import pandas as pd                                             # dataframes\n",
    "\n",
    "# python standard library\n",
    "from time import time                                           # used for timing execution\n",
    "from datetime import date, datetime                             # get current data and time\n",
    "import json                                                     # read/ write json\n",
    "import re                                                       # regex search \n",
    "import os                                                       # os operations\n",
    "import random                                                   # randomness\n",
    "from collections import Counter                                 # efficient counting\n",
    "import contextlib\n",
    "\n",
    "# custom imports\n",
    "from cscripts import metrics\n",
    "from cscripts import plotting\n",
    "from cscripts import summarise\n",
    "from cscripts import backboning # michele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set global style of plots\n",
    "\n",
    "Below you can specify global style for all plots or any other setups related to plots visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={\"xtick.bottom\" : True, \"ytick.left\" : True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-91fbf149-54bb-4103-8781-05f116ed2e73",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Flags\n",
    "\n",
    "Flags are used to control the run flow of the notebook when executed at once. This is useful, to prevent operations that should only produce a result once, from running multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-53bd492d-9bcd-4b38-aa38-b0a5ded0f905",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1633939929824,
    "source_hash": "76037a73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# section flags\n",
    "LOAD_DATA = True # Loads raw data for initial inspection\n",
    "TRANSFORM_DATA = False # Transforms raw data into more suitable format (Load data needs to be on as well)\n",
    "COMPUTE_PROJECTIONS = False\n",
    "GENERATE_SUMMARY = True\n",
    "DO_BACKBONING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-38436e51-9336-4e56-98ee-f9499a210276",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-22b35ef9-6971-4ab9-a837-1ac0c1f82e62",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1633939929836,
    "source_hash": "7708a6"
   },
   "outputs": [],
   "source": [
    "PATH_TO = {}\n",
    "PATH_TO['data'] = {}\n",
    "PATH_TO['data']['raw'] = 'data/raw'\n",
    "PATH_TO['data']['transformed'] = 'data/transformed'\n",
    "PATH_TO['data']['projections'] = 'data/projections'\n",
    "PATH_TO['data']['backboning'] = 'data/backboning'\n",
    "PATH_TO['data']['figures'] = 'data/figures'\n",
    "PATH_TO['data']['summaries'] = 'data/graph_summaries'\n",
    "PATH_TO['data']['metadata'] = 'data/metadata'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-3d5d8dac-f82c-4a86-ba1d-5b84a95d7552",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Folder Structure\n",
    "\n",
    "Create relevant folders to read from and write to, if not yet existent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-08e3a7d1-6b4d-4695-a69d-33f4b46e040e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 484498,
    "execution_start": 1633939929852,
    "source_hash": "84ce9148",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate over path_to dict\n",
    "for path in PATH_TO['data'].values():\n",
    "    os.makedirs(path) if not os.path.exists(path) else print('Already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-ad4ff0e0-c3b4-4f28-9e82-37ff1be70da7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## #01 Loading and Inspecting Raw Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-b9704f80-1b37-4859-8f1b-fec63d62a33a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 236,
    "execution_start": 1633939929892,
    "source_hash": "ec6986c"
   },
   "outputs": [],
   "source": [
    "if LOAD_DATA:\n",
    "    data = pd.read_csv(f\"{PATH_TO['data']['raw']}/data.txt\", delimiter=\":\", names=[\"user_id\", \"repo_id\"])\n",
    "    repos = pd.read_csv(f\"{PATH_TO['data']['raw']}/repos.txt\", delimiter=\":\", names=[\"repo_id\", \"meta_info\"])\n",
    "    lang = pd.read_csv(f\"{PATH_TO['data']['raw']}/lang.txt\", delimiter=\":\", names=[\"repo_id\", \"meta_info\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-a492c094-5e5a-4281-83a9-756fda0ba4da",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### `data.txt`\n",
    "\n",
    "This is the main dataset.  Each line is of the format `<user_id>`:`<repo_id>`\n",
    "which represents a user watching a repository.  There are 440,237 records\n",
    "in this file, each a single `user_id` and a single `repository_id` seperated by a colon. This file, thus, represents the bipartite graph of users following repositories as an edge list. The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-4ab9f25a-43ff-4672-b4ad-c12ccdfd85a3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 109,
    "execution_start": 1633939930134,
    "source_hash": "5d012fde",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Size of data.txt: {data.shape}\")\n",
    "print(f\"Number of Unique Users: {len(np.unique(data['user_id']))}\")\n",
    "print(f\"Number of Unique Repos: {len(np.unique(data['repo_id']))}\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-9e47b717-6ef6-4113-bead-d2eaaf1e8558",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### `repos.txt`\n",
    "\n",
    "This file lists out the 120,867 unique repositories using their id (`repo_id`) that are used in the `data.txt`\n",
    "set, providing the repository name, date it was created and (if applicable)\n",
    "the repository id that it was forked off of.  The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00015-d3554273-2334-404d-bf64-ad204ac9d8b3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 410,
    "execution_start": 1633939930220,
    "source_hash": "5be93304",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Size of repo.txt: {repos.shape}\")\n",
    "print(f\"ID's of data.txt and repos.txt match: {sum(np.unique(data['repo_id']) == np.unique(repos['repo_id'])) == 120867}\")\n",
    "print(f\"No. of Repos with forking info: {sum(repos['meta_info'].apply(lambda x: len(x.split(','))) == 3)}\")\n",
    "\n",
    "repos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-791b6bc2-f446-4171-a9a7-c6bddca22a5a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### `lang.txt`\n",
    "\n",
    "The last dataset included is the language breakdown data.  This lists  only contains 73,496 repositories, for which the language data was available. Each line of this file lists the repository id, then a comma delimited list of \n",
    "`<lang>`;`<lines>` entries containing each major language found and the number\n",
    "of lines of code for that language in the project.  The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-7ce72ccc-d5b0-43b5-bf04-34e1cea31259",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 65,
    "execution_start": 1633939930612,
    "source_hash": "5868930",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Size of repo.txt: {lang.shape}\")\n",
    "\n",
    "lang.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-0eb44930-382c-46fa-ad1e-b79e26933056",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## #02 Transforming Data\n",
    "---\n",
    "\n",
    "The goal of this section is to get the following two files:\n",
    "- data.txt = edge list containing our bipartite network\n",
    "- metadata.json = json file containing metadata about each repository within our bipartite network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-43286825-c940-40cf-8f2d-4eb121e22cab",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "###  Adding Class to Nodes in Edge List \n",
    "\n",
    "In the edge list `data.txt` the edges connecting a node of type *User* (from now referred to as $U$) to a node of type *Repository* (from now referred to as $R$) cannot be differentiated on each row. (e.g. 1:1 -> we need to denote that one on the left belongs to user and the other 1 to repository) To read in the graph object correctly at a later point, we therefore add correct labeling into that dataframe by adding `u` to user ids and `r` to repo ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-da5fbbdc-c809-498b-93a8-0b4fdccd65a0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1633939930653,
    "source_hash": "664ecae9"
   },
   "outputs": [],
   "source": [
    "if TRANSFORM_DATA:\n",
    "    data[\"user_id\"] = data[\"user_id\"].apply(lambda x: \"u\" + str(x))\n",
    "    data[\"repo_id\"] = data[\"repo_id\"].apply(lambda x: \"r\" + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-237c3979-38e5-48ec-9e31-58d20b850517",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Create Metadata\n",
    "\n",
    "Here we combine `repos.txt` and `lang.txt` into one json file where keys are `repo ids` and values are corresponding `metadata`. Note that `repos.txt` should have info about all repos in our network, however `lang.txt` has only info about a subset of repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00009-385e0618-75a4-475e-a7b1-e6239c11a736",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 484481,
    "execution_start": 1633939930653,
    "source_hash": "7d10674f"
   },
   "outputs": [],
   "source": [
    "if TRANSFORM_DATA:\n",
    "    # save repo metadata in dictionary\n",
    "    metadata = {}\n",
    "    repo_lang_mismatches = [] # Includes IDs of repos present in lang.txt but not in repos.txt\n",
    "\n",
    "    # save metadata from repos.txt\n",
    "    for i in range(len(repos)):\n",
    "        repo_id, meta_info = repos.iloc[i]\n",
    "\n",
    "        # parse the meta info\n",
    "        if len(meta_info.split(',')) == 2:\n",
    "            repo_name, creation_date, forked_from_id = meta_info.split(',') + [None]\n",
    "        else:\n",
    "            # you had None here at index 2 as well (not intended?)\n",
    "            repo_name, creation_date, forked_from_id = meta_info.split(',')\n",
    "\n",
    "        # save to dict\n",
    "        metadata[f\"r{repo_id}\"] = {\n",
    "            \"repo_name\": repo_name,\n",
    "            \"creation_date\": creation_date,\n",
    "            \"forked_from_id\": forked_from_id\n",
    "        }\n",
    "    \n",
    "    # save metadata from lang.txt\n",
    "    for i in range(len(lang)):\n",
    "        repo_id, lang_info = lang.iloc[i]\n",
    "       \n",
    "        # save it\n",
    "        if f\"r{repo_id}\" in metadata.keys():\n",
    "            metadata[f\"r{repo_id}\"][\"languages\"] = [l.split(';') for l in lang_info.split(',')]\n",
    "        \n",
    "        # It means that there is a repository in lang.txt which is not present in repos.txt\n",
    "        else:\n",
    "            repo_lang_mismatches.append(repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-10ebb5ab-5813-42c7-b666-8f2752632fb7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Save Transformed Files\n",
    "\n",
    "For faster access later, the following code cell saves the transformed network data and meta data into the directory `data/transformed`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-9e2349d8-fd1b-495b-bfdf-0d2b8455fca7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 484482,
    "execution_start": 1633939930655,
    "source_hash": "382b4fa2"
   },
   "outputs": [],
   "source": [
    "if TRANSFORM_DATA:\n",
    "    # save edge list\n",
    "    data.to_csv(f\"{PATH_TO['data']['transformed']}/data.txt\", header=False, index=False)\n",
    "    \n",
    "    # Save mismatches\n",
    "    with open(f\"{PATH_TO['data']['transformed']}/mismatches.csv\", \"w\") as fp:\n",
    "        fp.write(\",\".join([str(repoid) for repoid in repo_lang_mismatches]))\n",
    "\n",
    "    # save metadata as json\n",
    "    with open(f\"{PATH_TO['data']['transformed']}/metadata.json\", \"w\") as fp:\n",
    "        json.dump(metadata, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #03 Network Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve problems with mismatches (repos.txt <> lang.txt)\n",
    "\n",
    "We start by double checking that indeed intersection of set of available repos and set of mismatch repos is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mismatches = repos present in lang.txt but not in repos.txt\n",
    "with open(f\"{PATH_TO['data']['transformed']}/mismatches.csv\", \"r\") as fp:\n",
    "    mismatches = fp.readlines()[0].split(',')\n",
    "\n",
    "# Load all repos present\n",
    "repos = pd.read_csv(f\"{PATH_TO['data']['raw']}/repos.txt\", delimiter=\":\", names=[\"repo_id\", \"meta_info\"])\n",
    "\n",
    "# Double check that indeed intersection of set of available repos and set of mismatch repos is empty\n",
    "len(set(repos[\"repo_id\"]) & set(mismatches)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the description repos in `data.txt` should be all present within `repos.txt`. In other words, difference between these two sets should be an empty set. Let's check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from data.txt\n",
    "datatxt = pd.read_csv(f\"{PATH_TO['data']['raw']}/data.txt\", delimiter=\":\", names=[\"user_id\", \"repo_id\"])\n",
    "\n",
    "# Check the above mentioned assumption\n",
    "len(set(datatxt[\"repo_id\"]) - set(repos[\"repo_id\"])) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above two tests, we can safely conclude that mismatched **repos can be ignored.**\n",
    "\n",
    "### Solve problems nodes whose edges were randomly removed\n",
    "\n",
    "`Description of the problem:`\n",
    "\n",
    "I found out one important thing which we need to decide about. It is concerned with the stucture of our data. According to this website, the way the dataset was created was:\n",
    "- They retrieved all repo watches from their database (each edge then represents one watch in our network). Github was created back in 2008, therefore back then they ‚Äúonly‚Äù had around 0.5 million watches in db\n",
    "- From this set of watches, some of them were held back, i.e., removed from the dataset.\n",
    "- We were then provided a dataset of watches except from those that were withheld and from dataset called test.txt we know which users were impacted by this transformation. In other words, these users (roughly. 5000) have only a subset of their watches.\n",
    "- In total, there is around 56k users, and my question is to you what do we do with the 5k users about which we have incomplete information? (The reason why they withheld the watches is that the challenge of the contest was to predict these withheld watches‚Ä¶)\n",
    "\n",
    "`Proposed solution:`\n",
    "\n",
    "I think that it is reasonable to keep the affected users within the network and argue that we are working with a network which was adjusted by the approach described above, but most importantly, the above approach was random. In other words, the above approach for example did not focus on users which represent hubs. Another argument would be that this way, we do not lose information which we would have lost by dropping affected users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-035fa351-f075-4cb1-a9ae-c9a1387d46f0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## #04 Summarise Pre-Processed Bi-Partite Network\n",
    "---\n",
    "\n",
    "After having transformed the raw data in a format that is easily usable to be loaded as a graph object, we load our graph. We are using `networkX` - the standard library in Python for representation, visualisation and computation on graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00014-8dc92a4c-3417-49a8-92f6-cbbbf2c35588",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2153,
    "execution_start": 1633939930702,
    "source_hash": "810161b3"
   },
   "outputs": [],
   "source": [
    "# load graph\n",
    "G = nx.read_edgelist(f\"{PATH_TO['data']['transformed']}/data.txt\", delimiter=\",\", comments='#', create_using=nx.Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00015-eb621fa4-d684-4259-a78f-1208ab2ba26d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 997,
    "execution_start": 1633939932861,
    "source_hash": "5fc5b167"
   },
   "outputs": [],
   "source": [
    "# load metadata\n",
    "with open(f\"{PATH_TO['data']['transformed']}/metadata.json\", \"r\") as fp:\n",
    "    metadata = json.load(fp)\n",
    "\n",
    "# add metadata to graph nodes\n",
    "for repo_id, vals in metadata.items():\n",
    "    to_add = {repo_id: vals}\n",
    "    nx.set_node_attributes(G, to_add, \"metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00016-8356792f-9c49-4b17-9393-1f09256b5606",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 484353,
    "execution_start": 1633939933864,
    "source_hash": "30d7300e"
   },
   "outputs": [],
   "source": [
    "# Show a random node's attributes\n",
    "G.nodes[\"r2\"][\"metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00032-c0476895-cf1a-4b70-8514-1e5b8b38344e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Fundamental information\n",
    "\n",
    "Below, you can find fundamental information about our bi-partite network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00019-68f32ce5-95f3-457e-b358-f64b0b6216c8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2270,
    "execution_start": 1633939933887,
    "source_hash": "db990c54"
   },
   "outputs": [],
   "source": [
    "# Store fundamental properties within pandas DF\n",
    "overview = pd.DataFrame(columns=[\"name\", \"is_bipartite\", \"is_directed\", \"is_weighted\", \"selfloops_#\", \"nodes_#\", \"edges_#\", \"density\"])\n",
    "\n",
    "# save important data to dict\n",
    "G_info = dict()\n",
    "\n",
    "# fundamental stats\n",
    "G_info[\"name\"] = \"Github contest 2009\"\n",
    "G_info[\"number_of_users\"] = len([node for node in G.nodes() if node[0] == 'u'])\n",
    "G_info[\"number_of_repos\"] = len([node for node in G.nodes() if node[0] == 'r'])\n",
    "G_info[\"is_directed\"] = nx.is_directed(G)\n",
    "G_info[\"is_weighted\"] = nx.is_weighted(G)\n",
    "G_info[\"selfloops_#\"] = nx.number_of_selfloops(G)\n",
    "G_info[\"nodes_#\"] = nx.number_of_nodes(G)\n",
    "G_info[\"edges_#\"] = nx.number_of_edges(G)\n",
    "G_info[\"density\"] = nx.density(G)\n",
    "G_info[\"is_bipartite\"] = bool(nx.is_bipartite(G))\n",
    "\n",
    "# add it do the overview\n",
    "overview = overview.append(G_info, ignore_index=True)\n",
    "overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00036-39b21e1a-ea2b-4ef4-b205-572b9714d2c8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Top 10 repositories\n",
    "\n",
    "Below, we show 10 most popular repositories. Here are the interesting insights:\n",
    "- Each repository is written in Ruby, it is also the leading language in terms number of lines of code\n",
    "- The above perhaps corresponds to the fact that the top repository is rails which is a framework for web development for Ruby developers. Here is a relevant [article](https://syndicode.com/blog/why-is-ruby-still-our-choice-in-2020-2/) about Ruby popularity over time.\n",
    "- In addition, Github is developed using Ruby on Rails, perhaps that is where the popularity comes from. According to [this](https://gitstar-ranking.com/rails) site, Rails ranks 65th as of today.\n",
    "- Looking at other repositories, we can see that they correspond to web-development, e.g. authlogic or cucumber (platform for testing)\n",
    "- *(feel free to add more here)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-a4f41736-2948-48ac-874c-cd388e1ab01b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 732,
    "execution_start": 1633939936204,
    "source_hash": "72461587",
    "tags": []
   },
   "outputs": [],
   "source": [
    "repos_degree = sorted([(repo, G.degree(repo)) for repo in G.nodes() if repo[0] == 'r'], key=lambda x: x[1], reverse=True)[:10]\n",
    "repo_info = {\n",
    "    \"id\": [repo_tuple[0] for repo_tuple in repos_degree],\n",
    "    \"name\": [G.nodes[repo_tuple[0]][\"metadata\"][\"repo_name\"] for repo_tuple in repos_degree],\n",
    "    \"degree\": [repo_tuple[1] for repo_tuple in repos_degree],\n",
    "    \"technology_stack\": [G.nodes[repo_tuple[0]][\"metadata\"][\"languages\"] for repo_tuple in repos_degree]\n",
    "}\n",
    "top_10_repos = pd.DataFrame(repo_info)\n",
    "top_10_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most used programming languages\n",
    "\n",
    "Below, we can see most used programming languages in terms of package development. Here are the notes/thoughts about the below result:\n",
    "- we should be aware that this metric can be easily skewed by outliers, i.e., you can have one large repository which will contribute a lot\n",
    "\n",
    "- C, Ruby, C++ as top 3 languages is not a surprise since nowadays C is still one of the most popular programming languages, with regards to Ruby, see the above section\n",
    "- *(feel free to add more)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the usage in dict such that: key is repo_name and value is number of lines of code\n",
    "lang_usage = dict()\n",
    "\n",
    "# Fill lang_usage\n",
    "for info in metadata.values():\n",
    "    if \"languages\" in info:\n",
    "        for lang_info in info[\"languages\"]:\n",
    "            lang_name, n_lines = lang_info\n",
    "            if lang_name in lang_usage:\n",
    "                lang_usage[lang_name] += int(n_lines)\n",
    "            else:\n",
    "                lang_usage[lang_name] = int(n_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for plotting\n",
    "# * First fit it to the pd dataframe\n",
    "langs = np.array(list(lang_usage.keys())).reshape(-1, 1)\n",
    "n_lines = np.array(list(lang_usage.values()), dtype=int).reshape(-1, 1)\n",
    "df_lang_usage = pd.DataFrame(np.concatenate((langs, n_lines), axis=1), columns = [\"Lang\", \"# of lines\"])\n",
    "df_lang_usage['# of lines'] = df_lang_usage['# of lines'].astype('int64')\n",
    "\n",
    "# * Second sorted by # of lines in descending order (largest first)\n",
    "sorteddf = df_lang_usage.sort_values([\"# of lines\"], axis = 0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup figure so you can plot inside it\n",
    "fig = plt.figure(figsize=(7, 6)) # create figure object with a (width,height)\n",
    "ax = fig.add_axes([0.1,0.2,0.8, 0.7]) # left, bottom, width, height (range 0 to 1)\n",
    "\n",
    "# Plot the data\n",
    "sns_plot = sns.barplot(x=\"Lang\", y=\"# of lines\", data=sorteddf[:10], ax=ax, palette=\"rocket\");\n",
    "ax.tick_params(axis='x', rotation=90);\n",
    "\n",
    "# Add axis labels\n",
    "ax.set_ylabel('Number of lines of code');\n",
    "ax.set_title(\"Most used programming languages\", weight=\"bold\");\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(f\"{PATH_TO['data']['figures']}/most_used_lang.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00036-3d1e473c-b254-45c0-bf0c-3f7a83fbf19f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Degree distribution\n",
    "\n",
    "*Disclaimer: The code for this section is adapted from [this source](https://www.networkatlas.eu/exercise.htm?c=6&e=4).*\n",
    "\n",
    "Start with creating a dataframe where one column represents possible degree of nodes, and the other count of nodes which have such degree. In addition, sort it. Do this for both type of nodes.\n",
    "\n",
    "#### Get df for repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_degree = [G.degree(n) for n in G.nodes() if n[0] == 'r']\n",
    "dd_repos = Counter(repos_degree)\n",
    "dd_repos = pd.DataFrame(list(dd_repos.items()), columns = (\"k\", \"count\")).sort_values(by = \"k\")\n",
    "dd_repos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get df for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_degree = [G.degree(n) for n in G.nodes() if n[0] == 'u']\n",
    "dd_user = Counter(user_degree)\n",
    "dd_user = pd.DataFrame(list(dd_user.items()), columns = (\"k\", \"count\")).sort_values(by = \"k\")\n",
    "dd_user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize degree distribution (in terms of count) for both node types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define axs and figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.subplots_adjust(wspace=.3)\n",
    "\n",
    "# Plot the data\n",
    "for ax, data, which_category in zip(axs, [dd_repos, dd_user], [\"repositories\", \"users\"]):\n",
    "    sns.scatterplot(data=data, x=\"k\", y=\"count\", ax=ax, alpha=0.4)\n",
    "    ax.set_title(f\"Degree distribution for\\n{which_category} in terms of k\", weight = \"bold\")\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"{PATH_TO['data']['figures']}/dd_bipartite_count.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize degree distribution (in terms of count) on log-log scale for both node types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define axs and figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.subplots_adjust(wspace=.3)\n",
    "\n",
    "# Plot the data\n",
    "for ax, data, which_category in zip(axs, [dd_repos, dd_user], [\"repositories\", \"users\"]):\n",
    "    sns.scatterplot(data=data, x=\"k\", y=\"count\", ax=ax, alpha=0.4)\n",
    "    ax.set_title(f\"Degree distribution for\\n{which_category} in terms of k\", weight = \"bold\")\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"{PATH_TO['data']['figures']}/dd_bipartite_count_loglog.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get CCDF dataframe for repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdf_repos = dd_repos.sort_values(by = \"k\", ascending = False)\n",
    "ccdf_repos[\"cumsum\"] = ccdf_repos[\"count\"].cumsum()\n",
    "ccdf_repos[\"ccdf\"] = ccdf_repos[\"cumsum\"] / ccdf_repos[\"count\"].sum()\n",
    "ccdf_repos = ccdf_repos[[\"k\", \"ccdf\"]].sort_values(by = \"k\")\n",
    "ccdf_repos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get CCDF dataframe for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdf_users = dd_user.sort_values(by = \"k\", ascending = False)\n",
    "ccdf_users[\"cumsum\"] = ccdf_users[\"count\"].cumsum()\n",
    "ccdf_users[\"ccdf\"] = ccdf_users[\"cumsum\"] / ccdf_users[\"count\"].sum()\n",
    "ccdf_users = ccdf_users[[\"k\", \"ccdf\"]].sort_values(by = \"k\")\n",
    "ccdf_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize CCDF for both node types on a log-log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define axs and figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.subplots_adjust(wspace=.3)\n",
    "\n",
    "# Plot the data\n",
    "for ax, data, which_category in zip(axs, [ccdf_repos, ccdf_users], [\"repositories\", \"users\"]):\n",
    "    sns.lineplot(data=data, x=\"k\", y=\"ccdf\", ax=ax)\n",
    "    ax.set_title(f\"CCDF for {which_category}\", weight = \"bold\")\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"{PATH_TO['data']['figures']}/dd_bipartite_ccdf_loglog.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00038-1fd838d9-9510-4d37-ae0d-f79d6b1ab5ea",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Power law fit\n",
    "\n",
    "*Disclaimer: The code for this section is adapted from [this source](https://www.networkatlas.eu/exercise.htm?c=6&e=5).*\n",
    "\n",
    "*(Need to go over this section and write comments regards to the intepretation)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute power law fit for repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00039-9595dd92-59ef-4bd6-a029-f1835de01a8d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 475,
    "execution_start": 1633939939547,
    "source_hash": "3f80c4de",
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pl.Fit(ccdf_repos[\"ccdf\"])\n",
    "k_min = ccdf_repos[ccdf_repos[\"ccdf\"] == results.power_law.xmin][\"k\"]\n",
    "ccdf_repos[\"fit\"] = (10 ** results.power_law.Kappa) * (ccdf_repos[\"k\"] ** -results.power_law.alpha)\n",
    "print(\"Powerlaw CCDF Fit: %1.4f x ^ -%1.4f (k_min = %d)\" % (10 ** results.power_law.Kappa, results.power_law.alpha, k_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute power law fit for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pl.Fit(ccdf_users[\"ccdf\"])\n",
    "k_min = ccdf_users[ccdf_users[\"ccdf\"] == results.power_law.xmin][\"k\"]\n",
    "ccdf_users[\"fit\"] = (10 ** results.power_law.Kappa) * (ccdf_users[\"k\"] ** -results.power_law.alpha)\n",
    "print(\"Powerlaw CCDF Fit: %1.4f x ^ -%1.4f (k_min = %d)\" % (10 ** results.power_law.Kappa, results.power_law.alpha, k_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the fit for both node types\n",
    "\n",
    "The conclusion from the both plots is that there is now way that either of them follows power law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define axs and figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.subplots_adjust(wspace=.3)\n",
    "\n",
    "# Plot the data\n",
    "for ax, data, which_category in zip(axs, [ccdf_repos, ccdf_users], [\"repositories\", \"users\"]):\n",
    "    sns.lineplot(data=data, x=\"k\", y=\"ccdf\", ax=ax)\n",
    "    sns.lineplot(data=data, x=\"k\", y=\"fit\", ax=ax)\n",
    "    ax.set_title(f\"Powerlaw fit for {which_category}\", weight = \"bold\")\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(f\"{PATH_TO['data']['figures']}/power_law_fit_bipartite.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unused objects from memory\n",
    "del G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00036-7142cda8-9b08-4aab-8b82-c2f624e4309f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## #05 Projections\n",
    "---\n",
    "\n",
    "In this section, we focus on computing projections of our bipartite network onto repository nodes. The reason why we decided to choose repository nodes is that it makes more sense in the context of our problem, i.e., predicting relevant repositories to follow for the given user. In other words, in the projected network, two nodes will be connected with an edge if they have common neighbors, in this case github users. We use several projections method where each method puts different weight $w$ to given edge.\n",
    "\n",
    "### Prepare projection methods\n",
    "\n",
    "Within this section, we focus on implementing selected projection methods from the book. In addition, below, you can also find a short description of each method.\n",
    "\n",
    "#### Get set of users and repos in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-93bc004e-3614-4f7e-94cc-9eb25bb8fe2f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 30,
    "execution_start": 1633939940059,
    "source_hash": "165d96a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "repos = [node for node in G.nodes() if node[0] == 'r']\n",
    "users = [node for node in G.nodes() if node[0] == 'u']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define weight functions which do not require vectorization\n",
    "\n",
    "*(Here, we need to add description of the below methods)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_weight(G, u, v):\n",
    "    n_u, n_v = set(G[u]), set(G[v])\n",
    "    # len of set of intersection\n",
    "    return len(n_u & n_v) \n",
    "\n",
    "def jaccard(G, u, v):\n",
    "    n_u, n_v = set(G[u]), set(G[v])\n",
    "    # normalise len of set of intersection by union\n",
    "    return len(n_u & n_v) / len(n_u | n_v)\n",
    "\n",
    "def hyperbolic(G, u, v):\n",
    "    common = set(G[u]) & set(G[v])\n",
    "    return sum([1/(len(set(G[node])) - 1) for node in common])\n",
    "    \n",
    "def probs(G, u, v):  \n",
    "    common = set(G[u]) & set(G[v])\n",
    "    return sum([1/(len(set(G[node]))*len(set(G[u]))) for node in common])\n",
    "\n",
    "def heats(G, u, v):  \n",
    "    common = set(G[u]) & set(G[v])\n",
    "    return sum([1/(len(set(G[node]))*len(set(G[v]))) for node in common])\n",
    "\n",
    "def hybrid(G, u, v):  \n",
    "    common = set(G[u]) & set(G[v])\n",
    "    return sum([1/(len(set(G[node]))*len(set(G[u]))*len(set(G[v]))) for node in common])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define weight functions which require vectorization\n",
    "\n",
    "*(Here, we need to add description of the below methods)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorisedProjection:\n",
    "    def __init__(self, G, repos, users):\n",
    "        self.metric = None\n",
    "        self.adj = nx.algorithms.bipartite.matrix.biadjacency_matrix(G, repos, users)\n",
    "        self.repo_map = {repos[i]: i for i in range(len(repos))}\n",
    "\n",
    "    def project(self, G, u, v):\n",
    "        \n",
    "        v_u = np.ravel(self.adj.getrow(0).todense().sum(axis=0))\n",
    "        v_v = np.ravel(self.adj.getrow(1).todense().sum(axis=0))\n",
    "        \n",
    "        if self.metric == 'simple_weight':\n",
    "            return np.sum((v_u + v_v) == 2)\n",
    "\n",
    "        elif self.metric == 'euclidean':\n",
    "            return sp.spatial.distance.euclidean(v_u, v_v)  \n",
    "\n",
    "        elif self.metric == 'normalised_euclidean':\n",
    "            return 1 / (np.sqrt(np.sum(np.power((v_u - v_v),2))) + 1)\n",
    "\n",
    "        elif self.metric == 'pearson':\n",
    "            return sp.stats.stats.pearsonr(v_u, v_v)[0] + 1\n",
    "\n",
    "        elif self.metric == 'cosine':\n",
    "            return 1 - sp.spatial.distance.cosine(v_u, v_v)\n",
    "\n",
    "        else:\n",
    "            print(\"Please specify one of the following metrics: ['simple_weight', 'euclidean', 'normalised_euclidean', 'pearson', 'cosine'].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the implemented projection methods\n",
    "\n",
    "The purpose of the below test is to just show that the implemented methods run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00044-cc714010-15ce-4497-89d1-02abaf85cf8d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8349,
    "execution_start": 1633939942256,
    "source_hash": "5ed5a4ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define functions and their names to be tested\n",
    "weight_functions = [simple_weight, jaccard, VectorisedProjection, hyperbolic, probs, heats]\n",
    "func_names = [\"Simple weight\", \"Jaccard\", \"Vectorised projection\", \"Hyperbolic\", \"Probs\", \"Heats\"]\n",
    "\n",
    "# Select random nodes from our bipartite network for which we want to do the test\n",
    "u = 'r6'\n",
    "v = 'r3'\n",
    "\n",
    "# Run the test\n",
    "for f, name in zip(weight_functions, func_names):\n",
    "    if f == VectorisedProjection:\n",
    "        f = VectorisedProjection(G, repos, users)\n",
    "        for metric in ['simple_weight', 'euclidean', 'normalised_euclidean', 'pearson', 'cosine']:\n",
    "            f.metric = metric\n",
    "            print(f\"{name} ({metric}): {f.project(G, u, v)}\")\n",
    "    else: \n",
    "        print(f\"{name}: {f(G, u, v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the performance of the implemented methods\n",
    "\n",
    "The goal of this section is to test performance of implemented methods. Or in other words, we want to figure out which methods are able to finish within a realistic timeframe.\n",
    "\n",
    "#### Default benchmark\n",
    "\n",
    "We tried to use `simple weight` projection method on our network, and it finished within `10 min`. Below, we look at the average time which it takes to compute weight for one edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default benchmark, at this pace, it will compute within 10 min\n",
    "# Simple weight projection\n",
    "%timeit -n 2000 simple_weight(G, \"r1\", \"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods which use vectorization\n",
    "We found out that all methods which do not require vectorization are able to finish within reasonable timeframe. Below, we demonstrate perfomance of one of the selected methods which uses vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate second method for projection, this time using pearson correlation of two vectors to measure edge weight\n",
    "vp = VectorisedProjection(G, repos, users)\n",
    "vp.metric = \"pearson\"\n",
    "\n",
    "# Run the test -  we see that the ratio with the previous method is: 690/2.93 ~ 235x, i.e., 235x more time (~40 h)\n",
    "%timeit -n 2000 vp.project(G, \"r1\", \"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of projections\n",
    "\n",
    "Finally, after all testing and performance evaluation, we compute projection of our bi-partite network using following methods:\n",
    "- `Simple weight`\n",
    "- `Jaccard`\n",
    "- `Hyperbolic`\n",
    "- `Probs`\n",
    "- `Heats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_functions_names = [\"simple_weight\", \"jaccard\", \"hyperbolic\", \"probs\", \"heats\"]\n",
    "projection_functions = [simple_weight, jaccard, hyperbolic, probs, heats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00048-9c928f17-cf37-4b25-b790-03012da5a9b1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1633939950608,
    "output_cleared": true,
    "source_hash": "9d6d29e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if COMPUTE_PROJECTIONS:\n",
    "    for f, name in zip(projection_functions, projection_functions_names):\n",
    "        print(f\"Currently working on: {name}\")\n",
    "        try:\n",
    "            projected = bipartite.generic_weighted_projected_graph(G, nodes=repos, weight_function=f)\n",
    "            nx.readwrite.gpickle.write_gpickle(projected, f\"{PATH_TO['data']['projections']}/{name}.pickle\")\n",
    "        except Exception as e:\n",
    "            print(f\"Function: {name} - failed because\")\n",
    "            print(e)\n",
    "            print(\"-\"*10)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Projections\n",
    "\n",
    "In this section, we are trying to compute relevant statistics and visualise properties of the projections in a useful way in order to evaluate, which projection method might be the most reasonable to use for a recommendation system. We are using our custom function `generate_summary`, which generates a markdown file in the generated filepath for the desired unipartite graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_SUMMARY:\n",
    "    metadata = {}\n",
    "    \n",
    "    for f, name in zip(projection_functions, projection_functions_names):\n",
    "        metadata[name] = {}\n",
    "        metadata[name]['date'] = date.today().strftime('%d/%m/%y')\n",
    "        metadata[name]['time'] = datetime.now().strftime('%H:%M:%S')\n",
    "        print('='*10)\n",
    "        print(f\"Projection Method: {name.replace('_',' ').title()}\\n\")\n",
    "        \n",
    "        # load pickle file into graph\n",
    "        print(\"Loading Projection Graph\")\n",
    "        \n",
    "        start = time()\n",
    "        G = nx.read_gpickle(f\"{PATH_TO['data']['projections']}/pickle_format/{name}.pickle\")\n",
    "        load_time = round(time() - start, 2)\n",
    "        metadata[name]['load_time'] = load_time\n",
    "\n",
    "        print(f\"Loaded in {load_time}s\\n\")\n",
    "        print(f\"Starting Markdown Generation\")\n",
    "        \n",
    "        start = time()\n",
    "        with contextlib.redirect_stdout(None): # surpress output\n",
    "            summarise.generate_summary(G, filepath=PATH_TO['data']['summaries'], name=name)\n",
    "        execution_time = round(time() - start, 2)\n",
    "        metadata[name]['execution_time'] = execution_time\n",
    "            \n",
    "        print(f\"Finished in {execution_time}s. Summary saved to {PATH_TO['data']['summaries']}/{name}\")\n",
    "        print('='*10 + '\\n')\n",
    "        metadata[name]['success'] = True\n",
    "    \n",
    "    filepath = f\"{PATH_TO['data']['metadata']}/projection_summaries.txt\"\n",
    "    with open(filepath, 'w') as outfile:\n",
    "        outfile.write('=== METADATA: Generating Summaries of Projections ===\\n\\n\\n')\n",
    "        for method in metadata:\n",
    "            outfile.write(f\"Projection Method: {method.replace('_', ' ').title()}\\n\")\n",
    "            outfile.write(f\"{'-'*len(method)}\\n\")\n",
    "            for stat, res in metadata[method].items():\n",
    "                outfile.write(f\"{stat.replace('_', ' ').title()}: {res}\\n\")\n",
    "            outfile.write('\\n')\n",
    "    print('Saved Metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #06 Network backboning\n",
    "---\n",
    "\n",
    "Since our projected graphs are pretty dense, we need to use network backboning in order to filter out irrelevant edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why can not we just filter out edges with weight less than certain threshold?\n",
    "\n",
    "Let's start with simple inspection of edge weight distribution.\n",
    "\n",
    "#### Get weights for graph obtained via simple weight projection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph obtained by simple weight projetion method\n",
    "G_simple = nx.readwrite.gpickle.read_gpickle(f\"{PATH_TO['data']['projections']}/pickle_format/simple_weight.pickle\")\n",
    "\n",
    "# Get weights within the graph\n",
    "weights = Counter([w for n1, n2, w in G_simple.edges.data(\"weight\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the weights distribution\n",
    "\n",
    "Below, you can see the first problem associated with using naive approach, i.e., to just get rid of nodes under certain nodes. The problem is broad weight distribution. (p. 333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup figure so you can plot inside it\n",
    "fig = plt.figure(figsize=(6, 4)) # create figure object with a (width,height)\n",
    "ax = fig.add_axes([0.15,0.2,0.8, 0.7]) # left, bottom, width, height (range 0 to 1)\n",
    "\n",
    "# Plot the distribution\n",
    "weight_dist = sns.scatterplot(x=weights.keys(), y=weights.values(), alpha=0.5, ax=ax)\n",
    "weight_dist.set(xscale=\"log\", yscale=\"log\", xlabel=\"Edge weight\", ylabel=\"# of edges\");\n",
    "weight_dist.set_title(\"Edge weight distribution\", weight=\"bold\");\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f\"{PATH_TO['data']['figures']}/weight_dist_simple_proj.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete G_simple from memory\n",
    "del G_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for backboning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now import a library provided by Michele on his web, which implements all the backboning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cscripts import backboning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to use the library, you need to provide in the specific format:\n",
    "- edge list\n",
    "- header must have columns src, trg, weight\n",
    "- should be tab separated\n",
    "- In case of undirected network, the edges have to be present in both directions with the same weights, or set triangular_input to True.\n",
    "\n",
    "With this in mind, I therefore saved our projections in this format. Note that by using edge list format, we lose nodes which are not connected to any other node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform backboning for selected projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have loaded the data in the proper format, we can do the network backboning. I chose the below two methods because they are computaionally viable. For instance Double stochastic method was not viable since we were not able to reach convergence. As Michele wrote in the book, this is likely because our network is sparse. The other methods are very computationally intensive (we can try and see if it works, but I doubt it will run in reasonable time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backboning_functions_names = ['disparity_filter', 'noise_corrected']\n",
    "backboning_functions = [backboning.disparity_filter, backboning.noise_corrected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_BACKBONING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_BACKBONING:\n",
    "    metadata = {}\n",
    "    for projection_name in projection_functions_names:\n",
    "        print('='*10)\n",
    "        print(f\"Projection Method: {name.replace('_',' ').title()}\\n\")\n",
    "        print('Loading Graph from edge list')\n",
    "        # Load the data into a table --> needed format for the library. FYI: _, _ = nnnodes, nnedges\n",
    "        start = time()\n",
    "        table, _, _ = backboning.read(f\"{PATH_TO['data']['projections']}/edge_list_format/{projection_name}.edges\",\n",
    "                                         \"weight\",\n",
    "                                         triangular_input=True,\n",
    "                                         undirected=True,\n",
    "                                         consider_self_loops=False)\n",
    "        load_time = round(time() - start, 2)\n",
    "        print(f'Loaded in {load_time}s\\n')\n",
    "        \n",
    "        for name, backboning_function in zip(backboning_functions_names, backboning_functions):\n",
    "            metadata[f'{projection_name} {name}'] = {}\n",
    "            metadata[f'{projection_name} {name}']['date'] = date.today().strftime('%d/%m/%y')\n",
    "            metadata[f'{projection_name} {name}']['time'] = datetime.now().strftime('%H:%M:%S')\n",
    "            metadata[f'{projection_name} {name}']['load_time'] = load_time\n",
    "            # compute backboned graph and save \n",
    "            start = ()\n",
    "            print('Starting Backboning')\n",
    "            backboning_function(table, undirected=True, return_self_loops=False).to_csv(f\"{PATH_TO['data']['backboning']}/{name}/{projection_name}.csv\", index=False)\n",
    "            execution_time = round(time() - start, 2)\n",
    "            print(f'Finished Backbning in {execution_time}s')\n",
    "            \n",
    "            metadata[f'{projection_name} {name}']['execution_time'] = execution_time\n",
    "            metadata[f'{projection_name} {name}']['success'] = True\n",
    "    \n",
    "    filepath = f\"{PATH_TO['data']['metadata']}/backboning.txt\"\n",
    "    with open(filepath, 'w') as outfile:\n",
    "        outfile.write('=== METADATA: Backboning ===\\n\\n\\n')\n",
    "        for method in metadata:\n",
    "            outfile.write(f\"Projection Method: {method.replace('_', ' ').title()}\\n\")\n",
    "            outfile.write(f\"{'-'*len(method)}\\n\")\n",
    "            for stat, res in metadata[method].items():\n",
    "                outfile.write(f\"{stat.replace('_', ' ').title()}: {res}\\n\")\n",
    "            outfile.write('\\n')\n",
    "    print('Saved Metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #07 Analysis of projected graphs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a threshold to filter out edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, select a signifficance score to filter out irrelevant edges from projected graph. The below `SIGNIF_THRS` as follows: `P-value` = 1 - `SIGNIF_THRS`. (Keep only edges whose `P-value` is higher than the `P-value` computed based on `SIGNIF_THRS`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNIF_THRS = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out edges and do analysis part\n",
    "\n",
    "Below, we first filter out irrelevant edges. Then compute summary of the given unipartite network and save it to markdown file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of available projections - must follow the naming as in data/backboning\n",
    "# To be added to the list: \"probs\", \"jaccard\", \"hyperbolic\", \"heats\"\n",
    "projection_names = [\"simple_weight\"]\n",
    "\n",
    "# Load the given table, filter out irrelevant edges, save it\n",
    "for projection_name in projection_names:\n",
    "    \n",
    "    # Disparity filter backboning\n",
    "    # * Filter\n",
    "    df = backboning.thresholding(\n",
    "        pd.read_csv(f\"{PATH_TO['data']['backboning']}/df_table_{projection_name}.csv\"),\n",
    "        SIGNIF_THRS)\n",
    "    \n",
    "    # * Analysis\n",
    "    # YOUR CODE GOES HERE MIKA\n",
    "    \n",
    "    \n",
    "    # Noise corrected backboning\n",
    "    df = backboning.thresholding(\n",
    "        pd.read_csv(f\"{PATH_TO['data']['backboning']}/nc_table_{projection_name}.csv\"),\n",
    "        SIGNIF_THRS)\n",
    "    \n",
    "    # * Analysis\n",
    "    # YOUR CODE GOES HERE MIKA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #08 Community discovery\n",
    "\n",
    "We decided to do community discovery for these selected projected - backboned networks \"....\" using random walk and label propagation. To evaluate the existence of communities we used Modularity, Coverage and Performance\n",
    "\n",
    "Modularity \n",
    "- NA book\n",
    "\n",
    "Coverage\n",
    "- The coverage of a partition is the ratio of the number of intra-community edges to the total number of edges in the graph.\n",
    "\n",
    "Performance\n",
    "- The performance of a partition is the number of intra-community edges plus inter-community non-edges divided by the total number of potential edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initiate a graph for the backboned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backboned_unipartite_name = 'df_table_simple_weight'\n",
    "edge_list = pd.read_csv(f\"{PATH_TO['data']['backboning']}/{backboned_unipartite_name}.csv\")\n",
    "G = nx.from_pandas_edgelist(edge_list,'src','trg', edge_attr='score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find communitites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cscripts import community_discovery\n",
    "\n",
    "method_name = 'label_prop_semi' # label_prop_semi, 'label_prop_asyn','max_modularity', 'random_walk'\n",
    "communities = community_discovery.CD_unipartite(G,method = method_name,weight_name='score')\n",
    "\n",
    "print(len(communities)) #number of communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure the existance of communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name = 'modularity' # modularity, coverage,performance\n",
    "\n",
    "metric = community_discovery.Partition_measure(G,communities ,method = method_name,weight_name='score')\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "5befc242-845e-4d7f-adb3-0e0bb747a3d1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
